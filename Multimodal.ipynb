{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP20AhNsaejv2Qu/QxFu+yt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5373811b91a4105a3f7c1d7046296ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18a12d7f5b44496b9d4a3b1d60241cab",
              "IPY_MODEL_4edcb944c27040d2870fb5853747693e",
              "IPY_MODEL_af197a33d603491b92613f8f77cab446"
            ],
            "layout": "IPY_MODEL_10ce7f54d9e54cea80a96f1e99d6b8cc"
          }
        },
        "18a12d7f5b44496b9d4a3b1d60241cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81e35ad6fe6f4e59aeaad8a75b915242",
            "placeholder": "​",
            "style": "IPY_MODEL_ed88227f7d69492a9279b5b96bea2426",
            "value": "config.json: "
          }
        },
        "4edcb944c27040d2870fb5853747693e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4aadfd191114d8b98b40ac905618df0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a115b1737e474eae8b0bfc9f2bb200b7",
            "value": 1
          }
        },
        "af197a33d603491b92613f8f77cab446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_135b6636c19c46c5ae5170d5fe072678",
            "placeholder": "​",
            "style": "IPY_MODEL_bc6fa665549847e6850d8590a6ee706e",
            "value": " 4.19k/? [00:00&lt;00:00, 270kB/s]"
          }
        },
        "10ce7f54d9e54cea80a96f1e99d6b8cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e35ad6fe6f4e59aeaad8a75b915242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed88227f7d69492a9279b5b96bea2426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4aadfd191114d8b98b40ac905618df0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a115b1737e474eae8b0bfc9f2bb200b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "135b6636c19c46c5ae5170d5fe072678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc6fa665549847e6850d8590a6ee706e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa5b7ac2caa94411be37eeb56eeda065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4e8706c1b464a539dc504f1d8c4e8dd",
              "IPY_MODEL_9efb629ad2fa419aa5bd4e6381b109ab",
              "IPY_MODEL_8632c19f51364b4bb5bbaebd4caaa968"
            ],
            "layout": "IPY_MODEL_d30d579c072c4996af449276e3e06c5a"
          }
        },
        "c4e8706c1b464a539dc504f1d8c4e8dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_471878638d954e08bd8db8ae07ee5e58",
            "placeholder": "​",
            "style": "IPY_MODEL_e6b219fd2019429d8e64f6651cf4febe",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "9efb629ad2fa419aa5bd4e6381b109ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e0cf9966d34321a1f7954d9881a88e",
            "max": 605247071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e8aebbd1c514ee3b173a04eebcece25",
            "value": 605247071
          }
        },
        "8632c19f51364b4bb5bbaebd4caaa968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_424d08f51cc8452ba5626dcc6ddc49ce",
            "placeholder": "​",
            "style": "IPY_MODEL_cb7efba0eaae4eb685ee2d0da525d20d",
            "value": " 605M/605M [00:09&lt;00:00, 43.7MB/s]"
          }
        },
        "d30d579c072c4996af449276e3e06c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471878638d954e08bd8db8ae07ee5e58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b219fd2019429d8e64f6651cf4febe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00e0cf9966d34321a1f7954d9881a88e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e8aebbd1c514ee3b173a04eebcece25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "424d08f51cc8452ba5626dcc6ddc49ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb7efba0eaae4eb685ee2d0da525d20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5edbe0389fba483196ca4323121a0daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1718ba09589e43c7a7f6299f0db284e8",
              "IPY_MODEL_72e4d4ec7a5047a895df5d5a58a1a310",
              "IPY_MODEL_79261d6ddc0f48dd822229eea8e621e4"
            ],
            "layout": "IPY_MODEL_8a3756aed5484bb99cf44c55e501a1fb"
          }
        },
        "1718ba09589e43c7a7f6299f0db284e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_649a34ccfed6446f8f34a34cd97114f6",
            "placeholder": "​",
            "style": "IPY_MODEL_5b048ecc5d9749a3babff27147418866",
            "value": "model.safetensors: 100%"
          }
        },
        "72e4d4ec7a5047a895df5d5a58a1a310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0561c73a04a44af8f16667cb2b8948f",
            "max": 605157884,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c27eff6e018e49829245c263fe5a0728",
            "value": 605157884
          }
        },
        "79261d6ddc0f48dd822229eea8e621e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e100b727af243888ca0da1c57ec8f91",
            "placeholder": "​",
            "style": "IPY_MODEL_2b45417ae49545b6a76396164a042388",
            "value": " 605M/605M [00:07&lt;00:00, 142MB/s]"
          }
        },
        "8a3756aed5484bb99cf44c55e501a1fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "649a34ccfed6446f8f34a34cd97114f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b048ecc5d9749a3babff27147418866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0561c73a04a44af8f16667cb2b8948f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27eff6e018e49829245c263fe5a0728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e100b727af243888ca0da1c57ec8f91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b45417ae49545b6a76396164a042388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f19d8a124eeb47e39ae5be23a7119ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22190f1019944ccc9c93bac58c3bd99e",
              "IPY_MODEL_b54cb946444740bc9f6bb5f99a88ce32",
              "IPY_MODEL_28e25740626c464bb6a355957ceda825"
            ],
            "layout": "IPY_MODEL_df15077481e246e98518dbc07f439b6e"
          }
        },
        "22190f1019944ccc9c93bac58c3bd99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc89ff3e7da645a49a7a479282689252",
            "placeholder": "​",
            "style": "IPY_MODEL_d75593d90f4c4aa2a0421ad1c88679cc",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "b54cb946444740bc9f6bb5f99a88ce32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_122ad94f0b8545c29f092753ccfee2f1",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a17bddcf6e1c4ad482896a202df6a367",
            "value": 316
          }
        },
        "28e25740626c464bb6a355957ceda825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adccbf967a4e4b76b38b61d55ae99b93",
            "placeholder": "​",
            "style": "IPY_MODEL_37bd3b6142ee48df8879faeb14328d65",
            "value": " 316/316 [00:00&lt;00:00, 24.3kB/s]"
          }
        },
        "df15077481e246e98518dbc07f439b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc89ff3e7da645a49a7a479282689252": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d75593d90f4c4aa2a0421ad1c88679cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "122ad94f0b8545c29f092753ccfee2f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17bddcf6e1c4ad482896a202df6a367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adccbf967a4e4b76b38b61d55ae99b93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37bd3b6142ee48df8879faeb14328d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "771b53d6b4724dfdaac7ee271d231eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0222b1e7432f4774b44acbe9e0a5ef24",
              "IPY_MODEL_e60b01256fee422e9efdc5ceea92e36a",
              "IPY_MODEL_9bb55a29512945f4a317ee5f51b62153"
            ],
            "layout": "IPY_MODEL_980915fd2bf94f72b1d8b9733288cbe7"
          }
        },
        "0222b1e7432f4774b44acbe9e0a5ef24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a69ce22d34e433f92d0f3f1105fc8ab",
            "placeholder": "​",
            "style": "IPY_MODEL_1e4c3bdacc7244ddb982fa6a7c0fdbd7",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e60b01256fee422e9efdc5ceea92e36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a8467fdf9534da7a3a52187a11e2eb8",
            "max": 592,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dda6d68fee2e4b939942c8ab55e556db",
            "value": 592
          }
        },
        "9bb55a29512945f4a317ee5f51b62153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_069e4f9f71ad450dadf9d3e393b629a7",
            "placeholder": "​",
            "style": "IPY_MODEL_36c81d7263844b719b63d935a575f2fa",
            "value": " 592/592 [00:00&lt;00:00, 34.8kB/s]"
          }
        },
        "980915fd2bf94f72b1d8b9733288cbe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a69ce22d34e433f92d0f3f1105fc8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e4c3bdacc7244ddb982fa6a7c0fdbd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a8467fdf9534da7a3a52187a11e2eb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda6d68fee2e4b939942c8ab55e556db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "069e4f9f71ad450dadf9d3e393b629a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c81d7263844b719b63d935a575f2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cdb2f5b9a814db49838a497858a814d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df1da10c2f2e4cfeb6e718edda99e199",
              "IPY_MODEL_f7d026a9d1804d67b500da0a077873fc",
              "IPY_MODEL_c64088653c394938927a84249f9ff045"
            ],
            "layout": "IPY_MODEL_69e0d41b0658494ca0c25c018d6829d3"
          }
        },
        "df1da10c2f2e4cfeb6e718edda99e199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6601731adc6455e9aa139aa085b7f76",
            "placeholder": "​",
            "style": "IPY_MODEL_918b094f9d1b467fa5ea33d6f9734182",
            "value": "vocab.json: "
          }
        },
        "f7d026a9d1804d67b500da0a077873fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c777df2ec144493a2fe28ec818e54a5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2adabed2339477681ee53cbe38ab5f7",
            "value": 1
          }
        },
        "c64088653c394938927a84249f9ff045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc98f257be5d4f08a77e75d8fa2ca377",
            "placeholder": "​",
            "style": "IPY_MODEL_c707df70ad244f5c8d00b005176f4fef",
            "value": " 862k/? [00:00&lt;00:00, 19.7MB/s]"
          }
        },
        "69e0d41b0658494ca0c25c018d6829d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6601731adc6455e9aa139aa085b7f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "918b094f9d1b467fa5ea33d6f9734182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c777df2ec144493a2fe28ec818e54a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e2adabed2339477681ee53cbe38ab5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc98f257be5d4f08a77e75d8fa2ca377": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c707df70ad244f5c8d00b005176f4fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98d146e8e04241fa999ebf023e9dec2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_048a02124db246d486a4994ec3366430",
              "IPY_MODEL_e2fa184d864c4c219d74aabe1d929817",
              "IPY_MODEL_7b99c086e008454c963150ee5a153fe4"
            ],
            "layout": "IPY_MODEL_2aca4688ca704a37952fdc24f167ac5d"
          }
        },
        "048a02124db246d486a4994ec3366430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59dc8fb0648a4ce28269fd4fc383064b",
            "placeholder": "​",
            "style": "IPY_MODEL_ac686a569b9848e5a70cfa05c8015eeb",
            "value": "merges.txt: "
          }
        },
        "e2fa184d864c4c219d74aabe1d929817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc07ea894ed48828a143a0d890588bb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5d180650c2140ac9365ae83b42fd0e8",
            "value": 1
          }
        },
        "7b99c086e008454c963150ee5a153fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24bd0ed12e6a4c3e9a293f2b2e2304d2",
            "placeholder": "​",
            "style": "IPY_MODEL_bc1db8bca935401c9eb1b0b80bdfacd2",
            "value": " 525k/? [00:00&lt;00:00, 5.11MB/s]"
          }
        },
        "2aca4688ca704a37952fdc24f167ac5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59dc8fb0648a4ce28269fd4fc383064b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac686a569b9848e5a70cfa05c8015eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abc07ea894ed48828a143a0d890588bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e5d180650c2140ac9365ae83b42fd0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24bd0ed12e6a4c3e9a293f2b2e2304d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc1db8bca935401c9eb1b0b80bdfacd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04dc34e82b494c2384494134af0d711b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afc6a9c8e99342f2b8acb3a5184941b6",
              "IPY_MODEL_13d9607e1c31491199f1c2a302264c70",
              "IPY_MODEL_169b980bfa864b228171b3c65afb1519"
            ],
            "layout": "IPY_MODEL_eac6ae637022414b8d51136d948f43a3"
          }
        },
        "afc6a9c8e99342f2b8acb3a5184941b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ba3b0122ab649cca33199a070acd965",
            "placeholder": "​",
            "style": "IPY_MODEL_b598a30000214281a6ec25d36aca91f4",
            "value": "tokenizer.json: "
          }
        },
        "13d9607e1c31491199f1c2a302264c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0785b3294bab421987b984dbb6abeb12",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47333f581ec541d9bc481000001c5d12",
            "value": 1
          }
        },
        "169b980bfa864b228171b3c65afb1519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7910f965ea04f70ad8986cce2a1c3ed",
            "placeholder": "​",
            "style": "IPY_MODEL_15159085e91640c5918f1fc9b708cf66",
            "value": " 2.22M/? [00:01&lt;00:00, 2.16MB/s]"
          }
        },
        "eac6ae637022414b8d51136d948f43a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba3b0122ab649cca33199a070acd965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b598a30000214281a6ec25d36aca91f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0785b3294bab421987b984dbb6abeb12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "47333f581ec541d9bc481000001c5d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7910f965ea04f70ad8986cce2a1c3ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15159085e91640c5918f1fc9b708cf66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "badd1215cf8d4b48a49648dad87b6ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b847316e6b7d4da1bec30e3136d12bb2",
              "IPY_MODEL_49621dba8dc145a284f48bffbb37577a",
              "IPY_MODEL_ec70ce366d5d4b53b1a3e7ee2080a357"
            ],
            "layout": "IPY_MODEL_dacc94be90254b299a8e2d89e509178b"
          }
        },
        "b847316e6b7d4da1bec30e3136d12bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c55b6d82f5ce48f0b5ef5e274b1e7738",
            "placeholder": "​",
            "style": "IPY_MODEL_ec605c08b9b44aeea23320a4900d6fea",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "49621dba8dc145a284f48bffbb37577a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4504ff523d214d0b9efc09d5e1ac4ad9",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d5e99bb39474cde82cffcf5cf738098",
            "value": 389
          }
        },
        "ec70ce366d5d4b53b1a3e7ee2080a357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b6cc974bcc241d5910e14de5c6e31ed",
            "placeholder": "​",
            "style": "IPY_MODEL_accc6c2a0dc54020ade80c00bb87aff3",
            "value": " 389/389 [00:00&lt;00:00, 24.5kB/s]"
          }
        },
        "dacc94be90254b299a8e2d89e509178b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c55b6d82f5ce48f0b5ef5e274b1e7738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec605c08b9b44aeea23320a4900d6fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4504ff523d214d0b9efc09d5e1ac4ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d5e99bb39474cde82cffcf5cf738098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b6cc974bcc241d5910e14de5c6e31ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "accc6c2a0dc54020ade80c00bb87aff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitykundu/Simple_Rag_Project/blob/main/Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF langchain_community transformers torch numpy faiss-cpu langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWJ3k_a2gv8k",
        "outputId": "91707e0a-93a7-4df6-a5c5-a76a631eb1b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.9)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.13)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.18)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dcW7yYtBfp6w"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain.schema import Document\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.messages import HumanMessage\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS  # official LangChain FAISS\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Clip Model\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "## set up the environment\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "### initialize the Clip Model for unified embeddings\n",
        "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e5373811b91a4105a3f7c1d7046296ae",
            "18a12d7f5b44496b9d4a3b1d60241cab",
            "4edcb944c27040d2870fb5853747693e",
            "af197a33d603491b92613f8f77cab446",
            "10ce7f54d9e54cea80a96f1e99d6b8cc",
            "81e35ad6fe6f4e59aeaad8a75b915242",
            "ed88227f7d69492a9279b5b96bea2426",
            "b4aadfd191114d8b98b40ac905618df0",
            "a115b1737e474eae8b0bfc9f2bb200b7",
            "135b6636c19c46c5ae5170d5fe072678",
            "bc6fa665549847e6850d8590a6ee706e",
            "fa5b7ac2caa94411be37eeb56eeda065",
            "c4e8706c1b464a539dc504f1d8c4e8dd",
            "9efb629ad2fa419aa5bd4e6381b109ab",
            "8632c19f51364b4bb5bbaebd4caaa968",
            "d30d579c072c4996af449276e3e06c5a",
            "471878638d954e08bd8db8ae07ee5e58",
            "e6b219fd2019429d8e64f6651cf4febe",
            "00e0cf9966d34321a1f7954d9881a88e",
            "3e8aebbd1c514ee3b173a04eebcece25",
            "424d08f51cc8452ba5626dcc6ddc49ce",
            "cb7efba0eaae4eb685ee2d0da525d20d",
            "5edbe0389fba483196ca4323121a0daf",
            "1718ba09589e43c7a7f6299f0db284e8",
            "72e4d4ec7a5047a895df5d5a58a1a310",
            "79261d6ddc0f48dd822229eea8e621e4",
            "8a3756aed5484bb99cf44c55e501a1fb",
            "649a34ccfed6446f8f34a34cd97114f6",
            "5b048ecc5d9749a3babff27147418866",
            "c0561c73a04a44af8f16667cb2b8948f",
            "c27eff6e018e49829245c263fe5a0728",
            "8e100b727af243888ca0da1c57ec8f91",
            "2b45417ae49545b6a76396164a042388",
            "f19d8a124eeb47e39ae5be23a7119ab3",
            "22190f1019944ccc9c93bac58c3bd99e",
            "b54cb946444740bc9f6bb5f99a88ce32",
            "28e25740626c464bb6a355957ceda825",
            "df15077481e246e98518dbc07f439b6e",
            "dc89ff3e7da645a49a7a479282689252",
            "d75593d90f4c4aa2a0421ad1c88679cc",
            "122ad94f0b8545c29f092753ccfee2f1",
            "a17bddcf6e1c4ad482896a202df6a367",
            "adccbf967a4e4b76b38b61d55ae99b93",
            "37bd3b6142ee48df8879faeb14328d65",
            "771b53d6b4724dfdaac7ee271d231eba",
            "0222b1e7432f4774b44acbe9e0a5ef24",
            "e60b01256fee422e9efdc5ceea92e36a",
            "9bb55a29512945f4a317ee5f51b62153",
            "980915fd2bf94f72b1d8b9733288cbe7",
            "7a69ce22d34e433f92d0f3f1105fc8ab",
            "1e4c3bdacc7244ddb982fa6a7c0fdbd7",
            "2a8467fdf9534da7a3a52187a11e2eb8",
            "dda6d68fee2e4b939942c8ab55e556db",
            "069e4f9f71ad450dadf9d3e393b629a7",
            "36c81d7263844b719b63d935a575f2fa",
            "0cdb2f5b9a814db49838a497858a814d",
            "df1da10c2f2e4cfeb6e718edda99e199",
            "f7d026a9d1804d67b500da0a077873fc",
            "c64088653c394938927a84249f9ff045",
            "69e0d41b0658494ca0c25c018d6829d3",
            "e6601731adc6455e9aa139aa085b7f76",
            "918b094f9d1b467fa5ea33d6f9734182",
            "9c777df2ec144493a2fe28ec818e54a5",
            "e2adabed2339477681ee53cbe38ab5f7",
            "cc98f257be5d4f08a77e75d8fa2ca377",
            "c707df70ad244f5c8d00b005176f4fef",
            "98d146e8e04241fa999ebf023e9dec2a",
            "048a02124db246d486a4994ec3366430",
            "e2fa184d864c4c219d74aabe1d929817",
            "7b99c086e008454c963150ee5a153fe4",
            "2aca4688ca704a37952fdc24f167ac5d",
            "59dc8fb0648a4ce28269fd4fc383064b",
            "ac686a569b9848e5a70cfa05c8015eeb",
            "abc07ea894ed48828a143a0d890588bb",
            "e5d180650c2140ac9365ae83b42fd0e8",
            "24bd0ed12e6a4c3e9a293f2b2e2304d2",
            "bc1db8bca935401c9eb1b0b80bdfacd2",
            "04dc34e82b494c2384494134af0d711b",
            "afc6a9c8e99342f2b8acb3a5184941b6",
            "13d9607e1c31491199f1c2a302264c70",
            "169b980bfa864b228171b3c65afb1519",
            "eac6ae637022414b8d51136d948f43a3",
            "0ba3b0122ab649cca33199a070acd965",
            "b598a30000214281a6ec25d36aca91f4",
            "0785b3294bab421987b984dbb6abeb12",
            "47333f581ec541d9bc481000001c5d12",
            "a7910f965ea04f70ad8986cce2a1c3ed",
            "15159085e91640c5918f1fc9b708cf66",
            "badd1215cf8d4b48a49648dad87b6ed4",
            "b847316e6b7d4da1bec30e3136d12bb2",
            "49621dba8dc145a284f48bffbb37577a",
            "ec70ce366d5d4b53b1a3e7ee2080a357",
            "dacc94be90254b299a8e2d89e509178b",
            "c55b6d82f5ce48f0b5ef5e274b1e7738",
            "ec605c08b9b44aeea23320a4900d6fea",
            "4504ff523d214d0b9efc09d5e1ac4ad9",
            "4d5e99bb39474cde82cffcf5cf738098",
            "5b6cc974bcc241d5910e14de5c6e31ed",
            "accc6c2a0dc54020ade80c00bb87aff3"
          ]
        },
        "id": "Brgh3UGTiKar",
        "outputId": "fdcc4c35-2496-453f-ab1a-b84b2c43ab97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5373811b91a4105a3f7c1d7046296ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa5b7ac2caa94411be37eeb56eeda065"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5edbe0389fba483196ca4323121a0daf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f19d8a124eeb47e39ae5be23a7119ab3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "771b53d6b4724dfdaac7ee271d231eba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cdb2f5b9a814db49838a497858a814d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98d146e8e04241fa999ebf023e9dec2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04dc34e82b494c2384494134af0d711b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "badd1215cf8d4b48a49648dad87b6ed4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Embedding functions\n",
        "def embed_image(image_data):\n",
        "    \"\"\"Embed image using CLIP\"\"\"\n",
        "    if isinstance(image_data, str):  # If path\n",
        "        image = Image.open(image_data).convert(\"RGB\")\n",
        "    else:  # If PIL Image\n",
        "        image = image_data\n",
        "\n",
        "    inputs=clip_processor(images=image,return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_image_features(**inputs)\n",
        "        # Normalize embeddings to unit vector\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"Embed text using CLIP.\"\"\"\n",
        "    inputs = clip_processor(\n",
        "        text=text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=77  # CLIP's max token length\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_text_features(**inputs)\n",
        "        # Normalize embeddings\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()"
      ],
      "metadata": {
        "id": "Ei4OTf5OjB66"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Process PDF\n",
        "pdf_path=\"Rag.pdf\"\n",
        "doc=fitz.open(pdf_path)\n",
        "# Storage for all documents and embeddings\n",
        "all_docs = []\n",
        "all_embeddings = []\n",
        "image_data_store = {}  # Store actual image data for LLM\n",
        "\n",
        "# Text splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "O7edvwTAN9YC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arRpkV65ODCA",
        "outputId": "c7e5535f-fcba-4055-8ed7-55281c5862ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document('Rag.pdf')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,page in enumerate(doc):\n",
        "    ## process text\n",
        "    text=page.get_text()\n",
        "    if text.strip():\n",
        "        ##create temporary document for splitting\n",
        "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
        "        text_chunks = splitter.split_documents([temp_doc])\n",
        "\n",
        "        #Embed each chunk using CLIP\n",
        "        for chunk in text_chunks:\n",
        "            embedding = embed_text(chunk.page_content)\n",
        "            all_embeddings.append(embedding)\n",
        "            all_docs.append(chunk)\n",
        "\n",
        "\n",
        "\n",
        "    ## process images\n",
        "    ##Three Important Actions:\n",
        "\n",
        "    ##Convert PDF image to PIL format\n",
        "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
        "    ##Create CLIP embedding for retrieval\n",
        "\n",
        "    for img_index, img in enumerate(page.get_images(full=True)):\n",
        "        try:\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "\n",
        "            # Convert to PIL Image\n",
        "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "            # Create unique identifier\n",
        "            image_id = f\"page_{i}_img_{img_index}\"\n",
        "\n",
        "            # Store image as base64 for later use with GPT-4V\n",
        "            buffered = io.BytesIO()\n",
        "            pil_image.save(buffered, format=\"PNG\")\n",
        "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "            image_data_store[image_id] = img_base64\n",
        "\n",
        "            # Embed image using CLIP\n",
        "            embedding = embed_image(pil_image)\n",
        "            all_embeddings.append(embedding)\n",
        "\n",
        "            # Create document for image\n",
        "            image_doc = Document(\n",
        "                page_content=f\"[Image: {image_id}]\",\n",
        "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
        "            )\n",
        "            all_docs.append(image_doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "doc.close()"
      ],
      "metadata": {
        "id": "t0mmPFQaOIbY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz683H1LOMRf",
        "outputId": "19898025-714f-430a-fabe-a6d9d81343dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'type': 'text'}, page_content='ScienceDirect\\nAvailable online at www.sciencedirect.com\\nProcedia Computer Science 246 (2024) 3781–3790\\n1877-0509 © 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge \\nBased and Intelligent information and Engineering Systems\\n10.1016/j.procs.2024.09.178'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='Based and Intelligent information and Engineering Systems\\n10.1016/j.procs.2024.09.178\\nKeywords: Large Language Models (LLMs); Natural Language Processing (NLP); Retrieval-Augmented Generation (RAG); Text generation; \\nDigital transformation. \\n1. Introduction \\nDigital transformation signifies the incorporation of digital technology across different facets of a business, \\nreshaping its operations and value delivery to customers [1]. At the forefront of driving such transformative'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='practices are Large Language Models (LLMs), advanced machine learning models trained extensively on textual \\ndata to comprehend and produce human-like text [1]. LLMs, such as the Generative Pre-training Transformer (GPT) \\n \\n \\n* Corresponding author. Tel.: +33 03 80 39 50 00; fax: +33 03 80 39 50 69.  \\nE-mail address: muhammad.arslan@u-bourgogne.fr \\n28th International Conference on Knowledge-Based and Intelligent Information & Engineering \\nSystems (KES 2024) \\nA Survey on RAG with LLMs'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='Systems (KES 2024) \\nA Survey on RAG with LLMs \\nMuhammad Arslana*, Hussam Ghanema, Saba Munawarb and Christophe Cruza \\naLaboratoire Interdisciplinaire Carnot de Bourgogne (ICB), Dijon, France \\nbNational University of Computer and Emerging Sciences (NUCES), Islamabad, Pakistan                                                                 \\nAbstract \\nIn the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='competitive and foster growth. Large Language Models (LLMs) have emerged as game-changers across industries, \\nrevolutionizing various sectors by harnessing extensive text data to analyze and generate human-like text. Despite their \\nimpressive capabilities, LLMs often encounter challenges when dealing with domain-specific queries, potentially leading to \\ninaccuracies in their outputs. In response, Retrieval-Augmented Generation (RAG) has emerged as a viable solution. By'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='seamlessly integrating external data retrieval into text generation processes, RAG aims to enhance the accuracy and relevance of \\nthe generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG, \\noverlooking a comprehensive exploration of its applications. This paper seeks to address this gap by providing a thorough review'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='of RAG applications, encompassing both task-specific and discipline-specific studies, while also outlining potential avenues for \\nfuture research. By shedding light on current RAG research and outlining future directions, this review aims to catalyze further \\nexploration and development in this dynamic field, thereby contributing to ongoing digital transformation efforts. \\n© 2024 The Authors. Published by Elsevier B.V.'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='© 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge Based and \\nIntelligent information and Engineering Systems'),\n",
              " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]'),\n",
              " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_1'}, page_content='[Image: page_0_img_1]'),\n",
              " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_2'}, page_content='[Image: page_0_img_2]'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='3782\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nseries [2, 3] and others, have demonstrated remarkable capabilities in NLP tasks [4]. However, these models face \\nchallenges when dealing with domain-specific queries, often generating inaccurate or irrelevant information, \\ncommonly referred to as “hallucinations”, particularly when data is sparse [5]. This limitation makes deploying'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='LLMs in real-world settings impractical, as the generated output may not be reliable [4].  \\nIn the middle of 2020, Lewis et al. [6] introduced RAG, a significant advancement in the field of LLMs for \\nimproving generative tasks (see Fig. 1 (a)). RAG incorporates an initial step where LLMs search an external data \\nsource to retrieve relevant information before producing text or answering questions. RAG addresses these'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='limitations by integrating external data retrieval into the generative process, thereby enhancing the accuracy and \\nrelevance of the generated output. By dynamically retrieving information from knowledge bases during inference, \\nRAG provides a more informed and evidence-based approach to language generation, significantly reducing the risk \\nof hallucinations and improving the overall quality of the generated text [4, 6]. This approach has the potential to'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='make LLMs more practical for real-world applications, as it ensures that the generated output is grounded in \\nretrieved evidence, leading to more reliable and accurate results. Fig. 1 (b) showcases how real-time business \\nsystems can leverage the RAG with LLM architecture. As an example, without RAG, the system lacks access to \\nreal-time or updated information. However, with RAG integration, leveraging external data sources such as news'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='articles, the system can respond to current business events, presenting opportunities for business intelligence \\nanalysts. \\n \\n                                  (a)                                                                                             (b) \\nFig. 1. (a) A generic RAG architecture, where users’ queries, potentially in different modalities (e.g., text, code, image, etc.), are inputted into'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='both the retriever and the generator. The retriever scans for relevant data sources in storage, while the generator engages with the retrieval \\noutcomes, ultimately generating results across various modalities [6]; Fig. 1. (b) illustrates how RAG integration with the LLM handles queries \\nthat fall outside the scope of the LLM’s training data.  \\nWhile the field of RAG has seen substantial growth, several online surveys [4, 7, 8, 9] have explored'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='technological advancements in RAG. Although these surveys provide valuable insights and references, they offer \\nonly a limited overview of RAG applications. To address this gap, this paper aims to provide an exhaustive \\noverview of RAG applications, including both task-specific and discipline-specific studies, as well as future \\ndirections. By highlighting the current state of RAG research and its potential future directions, this review aims to'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='inspire further investigation and development in this exciting field. \\nThe paper’s structure is as follows: Section 2 presents the adopted research methodology for this survey. In \\nSection 3, we provide an overview of RAG applications, followed by a detailed discussion in Section 4. The paper \\nconcludes in Section 5, summarizing the key findings and implications of the study. \\n2. Background'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='concludes in Section 5, summarizing the key findings and implications of the study. \\n2. Background \\nThe research method (see Fig. 2) employed in this paper involves a thorough review and analysis of research \\npublications related to RAG. The main objective is to identify and categorize its applications across various NLP \\ntasks and disciplines. The paper begins by collecting research publications specific to RAG, focusing on their'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='applications. Since the RAG with LLM domain is relatively new and emerging, with many studies available as pre-'),\n",
              " Document(metadata={'page': 1, 'type': 'image', 'image_id': 'page_1_img_0'}, page_content='[Image: page_1_img_0]'),\n",
              " Document(metadata={'page': 1, 'type': 'image', 'image_id': 'page_1_img_1'}, page_content='[Image: page_1_img_1]'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3783\\nprints online, limiting the search to platforms such as Scopus or IEEE would greatly reduce the number of studies. \\nTherefore, Google Scholar was utilized to access the studies on RAG. However, in cases where both pre-print and \\npublished versions of a study were available, the published version was chosen to cover the maximum number of'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='peer-reviewed studies. Each study underwent manual review to assess its comprehensiveness and depth, excluding \\nshort studies. It is important to note that the purpose of the survey is not to cover the most optimal studies, but rather \\nto provide an overview of how this field has attained significant attention in a short period, with researchers \\nexploring diverse application scenarios.  \\nThe keywords used to collect research publications included “retrieval augmented generation”, “RAG'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='The keywords used to collect research publications included “retrieval augmented generation”, “RAG \\napplications”, “generative models with retrieval”, “external data retrieval in text generation”, “enhancing text \\ngeneration with retrieval”, “integrating retrieval into generative models”, “external knowledge in text generation”, \\n“retrieval-based text generation”, “information retrieval for text generation”, and “contextualized retrieval in'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='language models”. These publications are then classified into two principal categories: task-based classification and \\ndiscipline-based classification. Task-based classification focuses on categorizing RAG studies according to their \\nexecution of information processing tasks, particularly within NLP. Conversely, discipline-based classification \\ncategorizes studies based on their application to specific domains. Under the task-based classification, the'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='publications are further subdivided into categories such as Question Answering (QA), Text Generation and \\nSummarization, Information Retrieval and Extraction, Text Analysis and Processing, Software Development and \\nMaintenance (SDM), Decision Making and Applications, and Other Categories. Similarly, under the discipline-\\nbased classification, the publications are further subdivided into categories such as Medical/Biomedical, Financial,'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='Educational, Technology and Software Development, Social and Communication, Literature, and Other Categories. \\nThese categories are selected based on an understanding of the context of the studies and the underlying problems \\nthey address. Within both classification methods, “software development” stands out as a common category. It \\ninvolves programming information processing tasks under task-based classification and encompasses systems for'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='developing various applications across different domains under discipline-based classification. Figure 3 illustrates \\nthe number of publications related to RAG applications from 2020 to February 2024. Specifically, there was a single \\npublication found in 2020, 6 publications in 2022, 28 publications in 2023, and 16 publications until February 2024, \\nindicating a growing interest and research activity in the field of RAG applications. \\nFig. 2. Research Method'),\n",
              " Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_0'}, page_content='[Image: page_2_img_0]'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='3784\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Evolution of Research Publications on RAG Applications \\n3. Applications of RAG with LLMs \\nUpon thorough examination of the selected papers focusing on RAG applications, we uncovered a vast array of \\ndiverse applications. These findings are distilled into a comprehensive table format (see Table 1), detailing three'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='crucial aspects: 1) Use case with RAG, 2) Used datasets/benchmarks, and 3) Application area. Noteworthy \\napplications span various domains, including biomedical, financial, and medical inquiries, alongside text \\nsummarization and book review generation. RAG’s versatility extends to commonsense QA, table-based queries, \\nand clinical decision-making, among others. It further encompasses educational decision making, textbook question'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='answering, and enterprise search functionalities. RAG is instrumental in sentiments classification, health education, \\nand generating biomedical explanations, while also enhancing user writing accuracy and speed. Its utility spans \\nhumanitarian assistance, generating informative dialogues, crafting realistic images and intricate plotlines, and much \\nmore.  \\nAdditionally, RAG aids in natural language QA, disease identification, and information extraction. It handles'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='decision-making tasks, hashtag management, hate speech detection, and scientific document classification. RAG \\nexcels in entity description generation, text correction, and SQL translation, while also enhancing open-domain QA \\nand professional knowledge inquiries. Also, it extends the capabilities of machine translation tasks beyond text-to-\\nSQL, such as neural text re-ranking [6]. Moreover, it supports multicultural enterprise queries, e-commerce'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='searches, and personalized dialogue systems. Furthermore, RAG facilitates event argument extraction, intelligence \\nreport generation, short-form QA, automated transactions, and private data handling. Lastly, it contributes to science \\nQA, clinical writing, and pharmaceutical regulatory compliance inquiries. \\nAfter compiling all the applications of RAG, the subsequent step involves categorizing them based on the'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='specific nature of the NLP tasks they tackle (see Table 2 and Fig. 4). From the compiled publications, it was \\nobserved that 20 studies were dedicated to QA, 6 to Text Generation and Summarization, 6 to Information Retrieval \\nand Extraction, 5 to Text Analysis and Processing, 4 to SDM, and 5 to Decision Making and Applications, while the \\nremaining 6 studies were classified under \"Other Categories.\" This classification is significant as it helps in'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='understanding the distribution and focus of RAG applications across different NLP tasks. Additionally, since RAG \\napplications span various disciplines, further classification (see Table 3 and Fig. 5) reveals that 9 publications were \\nrelated to Medical/Biomedical, 2 to Financial, 2 to Educational, 9 to Technology and Software Development, 7 to \\nSocial and Communication, and 3 to Literature, with the remaining falling into \"Other Categories\". \\nTable 1. Applications of RAG  \\nNo.'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='Table 1. Applications of RAG  \\nNo. \\nUse case with RAG \\nUsed datasets / benchmarks \\nApplication area \\n1 \\nMIRAGE: Medical information RAG [10]  \\nMedical QA datasets \\nBiomedical QA \\n2 \\nRAG for improved context accuracy [11]  \\nFinancial reports \\nFinancial QA \\n3 \\nRetrieval-augmented Electrocardiography (ECG) [12]  \\nCardiac symptoms and sleep apnea diagnosis \\nMedical QA \\n4 \\nRepresentative Vector Summarization (RVS) [13]  \\nPDFs, text documents, spreadsheets, etc.  \\nMedical text summarization \\n5'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='PDFs, text documents, spreadsheets, etc.  \\nMedical text summarization \\n5 \\nRetrieval-augmented controllable reviews [14] \\nAmazon book reviews \\nBook review generation'),\n",
              " Document(metadata={'page': 3, 'type': 'image', 'image_id': 'page_3_img_0'}, page_content='[Image: page_3_img_0]'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3785\\n6 \\nRetrieval-augmented knowledge graph reasoning [15] \\nCommonsense QA and OpenBookQA. \\nCommonsense QA \\n7 \\nAnswers from table corpus via RAG [16] \\nWikipedia data \\nTable QA  \\n8 \\nLiVersa: a liver disease specific LLM using RAG [17] \\nLiver Diseases  \\nMedical QA \\n9 \\nAlmanac: RAG for clinical medicine [18] \\nGuidelines and treatment recommendations. \\nClinical decision-making \\n10 \\nAssessment of tutoring practices [19]'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Clinical decision-making \\n10 \\nAssessment of tutoring practices [19] \\nDialogue transcripts from a middle-school.  \\nEducational decision making \\n11 \\nHandling out of domain scenarios [20] \\nLife science, earth science, etc. lessons.  \\nTextbook QA \\n12 \\nAutomated form filling [21] \\nRequest forms for IT projects \\nEnterprise search \\n13 \\nFinancial sentiment analysis [22]  \\nTwitter financial news and FiQA datasets \\nSentiments classification \\n14 \\nFrontline health worker capacity building [23]'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Sentiments classification \\n14 \\nFrontline health worker capacity building [23] \\nPregnancy-related guidelines \\nHealth education QA \\n15 \\nSelf-BioRAG: a framework for biomedical text [24] \\nBiomedical instruction sets \\nBiomedical Informatics \\n16 \\nHybrid RAG for real-time composition assistance [25] \\nWikiText-103, Enron Emails, etc.  \\nWriting speed and accuracy \\n17 \\nRAG-Fusion to obtain product information [26] \\nProduct datasheets \\nTechnical information QA \\n18'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='RAG-Fusion to obtain product information [26] \\nProduct datasheets \\nTechnical information QA \\n18 \\nCommit message generation for code intelligence [27]  \\nMCMD dataset  \\nSDM \\n19 \\nFloodBrain: Flood disaster reporting [28] \\nReliefWeb reports \\nHumanitarian assistance \\n20 \\nRich answer encoding [29] \\nMSMARCO QA and WoW dataset. \\nGenerative QA  \\n21 \\nText-to-image generator [30] \\nCOCO and WikiImages datasets. \\nRealistic images generation \\n22 \\nCode completion framework [31]'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='COCO and WikiImages datasets. \\nRealistic images generation \\n22 \\nCode completion framework [31] \\nCodeXGLUE and CodeNet datasets. \\nSDM \\n23 \\nComplex story generation framework [32] \\nIMDB movie details dataset \\nGenerate stories  \\n24 \\nTRAC: Trustworthy retrieval augmented chatbot [33] \\nNatural Question dataset \\nNatural QA \\n25 \\nClinfo.ai using scientific literature [34] \\nPubMed dataset \\nMedical QA \\n26 \\nRealGen for controllable traffic scenarios [35] \\nnuScenes dataset \\nCritical traffic scenarios \\n27'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='RealGen for controllable traffic scenarios [35] \\nnuScenes dataset \\nCritical traffic scenarios \\n27 \\nZero-shot disease phenotyping [36]  \\nClinical notes \\nIdentifying diseases \\n28 \\nRAP-Gen for automatic program repair [37]  \\nTFix, Defects4J, etc. datasets \\nSDM \\n29 \\nCode4UIE : retrieval-augmented code generation [38] \\nACE04, ACE05, CoNLL03, etc. datasets \\nInformation extraction  \\n30 \\nRAP: retrieval-augmented planning [39] \\nALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='30 \\nRAP: retrieval-augmented planning [39] \\nALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31 \\nRIGHT for mainstream hashtag recommendation [40] \\nTwitter and Weibo data.  \\nRetrieval-enhanced hashtags \\n32 \\nRAUCG for counter narrative generation for hate speech \\n[41] \\nMultitargetCONAN dataset \\nCombating hate speech \\n33 \\nWeakly-supervised scientific document classification \\n[42] \\nAGNews and MeSH datasets.  \\nScientific documents \\nclassification \\n34'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='[42] \\nAGNews and MeSH datasets.  \\nScientific documents \\nclassification \\n34 \\nrT5 for Chinese entity description generation [43] \\nXunZi and MengZi datasets.  \\nEntity description generation \\n35 \\nRSpell: domain adaptive Chinese spelling check [44]  \\nCSC dataset \\nText error correction \\n36 \\nXRICL: cross-lingual retrieval-augmented in-context \\nlearning for cross-lingual text-to-SQL semantic parsing \\n[45] \\nXSPIDER and XKAGGLE-DBQA datasets. \\nText-to-SQL translation \\n37'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='[45] \\nXSPIDER and XKAGGLE-DBQA datasets. \\nText-to-SQL translation \\n37 \\nSELF-RAG: learning to retrieve, generate, and \\ncritique through self-reflection [46] \\nOpen-Instruct processed data. \\nOpen-domain QA and fact \\nverification \\n38 \\nChatDOC with enhanced PDF structure recognition [47] \\nAcademic papers, financial reports, \\ntextbooks, and legislative materials \\nProfessional knowledge QA \\n39 \\nG-Retriever for textual graph understanding [48] \\nGraphQA (ExplaGraphs, SceneGraphs and \\nWebQSP)'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='G-Retriever for textual graph understanding [48] \\nGraphQA (ExplaGraphs, SceneGraphs and \\nWebQSP)  \\nChat with graphs \\n40 \\nEnhancing multilingual information retrieval in \\nmixed Human Resources (HR) environments [49] \\nHR standard operating procedures and \\nQuality Assurance (QA) documents \\nMulticultural enterprise QA \\n41 \\nDifferentiable RAG [50] \\nUser-clicked logs \\nE-commerce search (query \\nintent classification) \\n42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data \\nSDM \\n43 \\nUniMS-RAG: a unified multi-source RAG [52] \\nDuLeMon and KBP datasets \\nPersonalized dialogue \\nsystems \\n44 \\nRAG QA for event argument extraction [53] \\nACE 2005 and WikiEvent datasets \\nEvent argument (answer) \\nextraction \\n45 \\nFABULA: retrieval-augmented narrative construction \\n[54] \\nOntoNotes and Pile datasets \\nIntelligence report \\ngeneration \\n46 \\nTime-Aware Adaptive Retrieval (TA-ARE) [55]'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Intelligence report \\ngeneration \\n46 \\nTime-Aware Adaptive Retrieval (TA-ARE) [55] \\nRetrievalQA dataset \\nShort-form open-domain \\nQA \\n47 \\nCash transaction booking via RAG [56] \\nCash Management Software (CMS) \\ntransactions. \\nAutomated cash transaction \\nbooking \\n48 \\nRetrieval-Augmented Thought Process (RATP) [57] \\nBoolq and emrQA datasets. \\nQA with private data \\n49 \\nATLANTIC for interdisciplinary science [58] \\nS2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='S2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50 \\nWriting documents for clinical trials [59] \\nFDA guidance database, ClinicalTrials.gov, \\nand AACT database.  \\nClinical-related writing \\n51 \\nQA RAG model [60] \\nFDA Q&A datasets  \\nPharma industry regulatory \\ncompliance QA'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='3786\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n4. Discussion  \\nThe classification of RAG applications according to the specific NLP tasks they target holds significant \\nimportance for several reasons. Firstly, it offers valuable insights into the distribution and focus of RAG applications \\nacross various tasks within the field of NLP. By quantifying the number of studies dedicated to each task,'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='researchers gain a deeper understanding of where efforts and resources are predominantly concentrated within the \\nRAG domain. By analyzing the distribution of RAG applications, researchers can discern prevailing trends in \\nresearch interest and identify emerging areas of importance. The classification of RAG applications based on \\ndiscipline offers valuable insights into its widespread adoption across various domains. This classification not only'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='provides a comprehensive understanding of RAG’s applicability but also underscores its potential to revolutionize \\nvarious domains, thereby contributing significantly to the advancement of NLP technologies.  \\nWhile this survey offers a comprehensive overview of RAG applications across various NLP tasks and \\ndisciplines, it also has its limitations. 1) Given that RAG technology is still emerging, the majority of RAG-based'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='studies are available in pre-print formats on platforms like arXiv, lacking peer review. This raises questions about \\ntheir authenticity. 2) Additionally, the survey overlooks the technical implementation details and challenges \\nassociated with using RAG technology alongside open-source LLMs. Organizations may find RAG implementation \\ncostly if they do not opt for open-source LLM architectures, especially considering the expense of querying the'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='LLM via Application Programming Interface (API). 3) Furthermore, the performance of RAG concerning the \\nvolume and variety of datasets has not been discussed. Deploying RAG with large datasets of varying structures \\n(e.g., structured, semi-structured, or non-structured) may lead to processing delays, warranting further exploration \\nbefore selecting a RAG with LLM integrated solution for organizational deployment.'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='before selecting a RAG with LLM integrated solution for organizational deployment.  \\n4) Additionally, this survey did not cover the diverse range of RAG architectures and technologies available for \\nintegration with different LLMs. Future work should delve into these options to discuss how various RAG solutions \\ncan be adapted with LLMs for different NLP tasks and applications. 5) Furthermore, the survey did not address the'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='accuracy of information obtained from RAG with LLM solutions. It is essential to explore the reliability of these \\nsystems and assess the organizations’ dependency on their generated responses. LLMs often generate responses with \\nhigh confidence, making it challenging to evaluate the accuracy of the information provided. 6) While the survey \\nprimarily focuses on task-based and discipline-based applications of RAG, there is a need for further research to'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='explore ethical considerations associated with its usage, especially when dealing with sensitive datasets. For \\nexample, in the biomedical domain, RAG has the potential to accidentally expose private information to analysts, \\nraising concerns about data privacy and security. Additionally, in the legal domain, RAG may mistakeably reveal \\nprivileged information during document analysis, potentially violating client confidentiality and attorney-client'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='privilege. Therefore, future studies should delve deeper into these ethical implications to ensure responsible and \\nethical use of RAG technology across various domains. \\n5. Conclusion  \\nThis article offers a thorough examination of the applications of RAG with LLMs, showcasing their potential to \\ndrive digital transformation across diverse industries. Initially, it gathers the latest publications on RAG from online'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='repositories. These publications are then classified based on task-oriented and discipline-oriented criteria. A notable \\ntrend observed is the increasing number of research papers on RAG deposited in open-access sources, particularly \\nsince 2023. However, many works remain unpublished or are in the preprint stage, awaiting review by various \\njournals. A significant portion of these studies primarily focus on the task of QA in NLP. Conversely, there is a'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='noticeable gap in research exploring Entity Linking, an essential NLP task that contributes to knowledge graph \\ndevelopment. Addressing this gap could unlock numerous applications in the realm of linked data. Regarding \\ndisciplines, the majority of research applications are concentrated in the fields of Medical/Biomedical and \\nTechnology and Software Development. In contrast, disciplines such as Business and Agriculture receive'),\n",
              " Document(metadata={'page': 5, 'type': 'text'}, page_content='comparatively less attention. Future research endeavors should aim to bridge this gap by addressing the specific \\nneeds of these underrepresented disciplines.'),\n",
              " Document(metadata={'page': 6, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3787\\nTable 2. Task-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 4. Task-based classification of RAG applications with count of publications. The word cloud is generated based on the publication counts'),\n",
              " Document(metadata={'page': 6, 'type': 'text'}, page_content='listed under various headings in Table 2. \\n \\n \\n \\n \\n1) Question Answering (QA)  \\n- Biomedical QA [1] \\n- Financial QA [2] \\n- Medical QA [3] \\n- Commonsense QA [6] \\n- Textbook QA [11] \\n- Health education QA [14] \\n- Technical product information QA [17] \\n- Natural QA [24] \\n- Professional knowledge QA [38] \\n- Multicultural enterprise QA [40] \\n- Open-domain QA and fact verification [46] \\n- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29]'),\n",
              " Document(metadata={'page': 6, 'type': 'text'}, page_content='- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29] \\n- Pharma industry regulatory compliance QA [51] \\n- Science QA and document classification [49] \\n- Clinical-related writing [50] \\n- Personalized dialogue systems [43] \\n2) Text Generation and Summarization \\n- Medical text summarization [4] \\n- Book review generation [5] \\n- Biomedical Informatics [15] \\n- Generate stories with complex plots [23] \\n- Generate realistic and faithful images [21]'),\n",
              " Document(metadata={'page': 6, 'type': 'text'}, page_content='- Generate stories with complex plots [23] \\n- Generate realistic and faithful images [21] \\n- Entity description generation [34] \\n \\n3) Information Retrieval and Extraction \\n- Table QA [7] \\n- Enterprise search [12] \\n- Retrieval-enhanced hashtags [31] \\n- Information extraction [29] \\n- Event argument (answer) extraction [44] \\n- E-commerce search (query intent classification) [41] \\n4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35]'),\n",
              " Document(metadata={'page': 6, 'type': 'text'}, page_content='4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Scientific documents classification [33] \\n- Combating online hate speech [32] \\n \\n5) Software Development and \\nMaintenance \\n- Code intelligence [18] \\n- Code completion [22] \\n- Automatic program repair [28] \\n- Elevate low-code developer skills [42] \\n \\n6) Decision Making and Applications \\n- Clinical decision-making [9] \\n- Educational decision making [10]'),\n",
              " Document(metadata={'page': 6, 'type': 'text'}, page_content='- Clinical decision-making [9] \\n- Educational decision making [10] \\n- Decision-making applications [30] \\n- Automated cash transaction booking [47] \\n- Intelligence report generation [45] \\n \\n7) Other Categories: \\n- Editing and crafting diverse behaviors, \\nincluding critical traffic scenarios [26] \\n- Identifying diseases [27] \\n- Chat with graphs [39] \\nTask: (Count of Publications)  \\nQuestion Answering (QA): (20) \\nText Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6)'),\n",
              " Document(metadata={'page': 6, 'type': 'text'}, page_content='Text Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6) \\nText Analysis and Processing: (5) \\nSoftware Development and Maintenance: (4) \\nDecision Making and Applications: (5) \\nOther Categories: (6)'),\n",
              " Document(metadata={'page': 6, 'type': 'image', 'image_id': 'page_6_img_0'}, page_content='[Image: page_6_img_0]'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='3788\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nTable 3. Discipline-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 5. Discipline-based classification of RAG applications with count of publications. The word cloud is generated based on the publication'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='counts listed under various headings in Table 3. \\nAcknowledgements \\nThe authors thank the French Government and the National Research Agency (ANR) for their funding. \\nReferences \\n[1] Roumeliotis KI, Tselikas ND, & Nasiopoulos DK. (2024). “LLMs in e-commerce: a comparative analysis of GPT and LLaMA models in \\nproduct review evaluation,” Natural Language Processing Journal:1-6:100056.'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='product review evaluation,” Natural Language Processing Journal:1-6:100056. \\n[2] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). “Language Models are Few-Shot \\nLearners,” Advances in Neural Information Processing Systems 33 (NeurIPS 2020).  \\n[3] OpenAI, R. (2023). “Gpt-4 technical report,” arxiv 2303.08774. View in Article: 2(5). \\n1) Medical / Biomedical \\n- Biomedical QA [1] \\n- Medical QA [3] \\n- Medical text summarization [4]'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='1) Medical / Biomedical \\n- Biomedical QA [1] \\n- Medical QA [3] \\n- Medical text summarization [4] \\n- Health education QA [14] \\n- Identifying diseases [27] \\n- Clinical decision-making [9] \\n- Clinical-related writing [50] \\n- Science QA and scientific document classification [49] \\n- Pharma industry regulatory compliance QA [51] \\n2) Financial \\n- Financial QA [2] \\n- Automated cash transaction booking [47] \\n3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11]'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11] \\n4) Technology and Software Development \\n- Table QA [7] \\n- Technical product information QA [17] \\n- Software development and maintenance [18, 22, 28, 42] \\n- Generative QA and informative conversations [20] \\n- Information extraction [29] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Personalized dialogue systems [43] \\n- Event argument (answer) extraction [44] \\n5) Social and Communication'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='- Event argument (answer) extraction [44] \\n5) Social and Communication \\n- Commonsense QA [6] \\n- Sentiments classification [13] \\n- Combating online hate speech [32] \\n- Retrieval-enhanced hashtags [31] \\n- Humanitarian assistance [19] \\n- Chat with graphs [39] \\n- Multicultural enterprise QA [40] \\n6) Literature  \\n- Book review generation guided by reference documents [5] \\n- Enhance user writing speed and accuracy [16] \\n- Generate stories with complex plots [23] \\n7) Other Categories'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='- Generate stories with complex plots [23] \\n7) Other Categories   \\n- Enterprise search [12] \\n- Generate realistic and faithful images [21] \\n- Decision-making applications [30] \\n- Open-domain question answering and fact verification [37] \\n- Professional knowledge QA [38] \\n- Intelligence report generation [45] \\n- Short-form open-domain QA [46] \\n- Question answering with private data [48] \\n \\nDiscipline: (Count of Publications)  \\nMedical / Biomedical: (9) \\nFinancial: (2) \\nEducational: (2)'),\n",
              " Document(metadata={'page': 7, 'type': 'text'}, page_content='Discipline: (Count of Publications)  \\nMedical / Biomedical: (9) \\nFinancial: (2) \\nEducational: (2) \\nTechnology and Software Development: (9) \\nSocial and Communication: (7) \\nLiterature (3) \\nOther Categories: (8)'),\n",
              " Document(metadata={'page': 7, 'type': 'image', 'image_id': 'page_7_img_0'}, page_content='[Image: page_7_img_0]'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3789\\n[4] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., ... & Wang, H. (2023). “Retrieval-augmented generation for large language models: A \\nsurvey,” arXiv preprint arXiv:2312.10997. \\n[5] Kandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2023). “Large language models struggle to learn long-tail knowledge,” In \\nInternational Conference on Machine Learning. PMLR: 5696-15707.'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='International Conference on Machine Learning. PMLR: 5696-15707. \\n[6] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). “Retrieval-augmented generation for knowledge-\\nintensive nlp tasks,” Advances in Neural Information Processing Systems 33: 9459-9474. \\n[7] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). “A survey on retrieval-augmented text generation,” arXiv preprint arXiv:2202.01110.'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='[8] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., ... & Scialom, T. (2023). “Augmented language models: a \\nsurvey,” arXiv preprint arXiv:2302.07842. \\n[9] Zhao, R., Chen, H., Wang, W., Jiao, F., Do, X. L., Qin, C., ... & Joty, S. (2023). “Retrieving multimodal information for augmented \\ngeneration: A survey,” arXiv preprint arXiv:2303.10868.'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='generation: A survey,” arXiv preprint arXiv:2303.10868. \\n[10] Xiong, G., Jin, Q., Lu, Z., & Zhang, A. (2024). “Benchmarking retrieval-augmented generation for medicine,” arXiv preprint \\narXiv:2402.13178. \\n[11] Jimeno Yepes, A., You, Y., Milczek, J., Laverde, S., & Li, L. (2024). “Financial Report Chunking for Effective Retrieval Augmented \\nGeneration,” arXiv e-prints, arXiv-2402.'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='Generation,” arXiv e-prints, arXiv-2402. \\n[12] Yu, H., Guo, P., & Sano, A. (2023). “Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation,” \\nIn Machine Learning for Health (ML4H) PMLR: 650-663. \\n[13] Manathunga, S. S., & Illangasekara, Y. A. (2023). “Retrieval Augmented Generation and Representative Vector Summarization for large \\nunstructured textual data in Medical Education,” arXiv preprint arXiv:2308.00479.'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='unstructured textual data in Medical Education,” arXiv preprint arXiv:2308.00479. \\n[14] Kim, J., Choi, S., Amplayo, R. K., & Hwang, S. W. (2020). “Retrieval-augmented controllable review generation,” In Proceedings of the \\n28th International Conference on Computational Linguistics: 2284-2295. \\n[15] Sha, Y., Feng, Y., He, M., Liu, S., & Ji, Y. (2023). “Retrieval-augmented Knowledge Graph Reasoning for Commonsense Question'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='Answering,” Mathematics 11(15): 3269; https://doi.org/10.3390/math11153269.  \\n[16] Pan, F., Canim, M., Glass, M., Gliozzo, A., & Hendler, J. (2022). “End-to-End Table Question Answering via Retrieval-Augmented \\nGeneration,” arXiv preprint arXiv:2203.16714. \\n[17] Ge, J., Sun, S., Owens, J., Galvez, V., Gologorskaya, O., Lai, J. C., ... & Lai, K. (2023). “Development of a Liver Disease-Specific Large \\nLanguage Model Chat Interface using Retrieval Augmented Generation,” medRxiv.'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='Language Model Chat Interface using Retrieval Augmented Generation,” medRxiv. \\n[18] Zakka, C., Shad, R., Chaurasia, A., Dalal, A. R., Kim, J. L., Moor, M., ... & Hiesinger, W. (2024). “Almanac—retrieval-augmented language \\nmodels for clinical medicine,” NEJM AI 1(2), AIoa2300068. \\n[19] Han, Z. FeiFei, Lin, J., Gurung, A., Thomas, D. R., Chen, E., Borchers, C., Gupta, S., & Koedinger, K. R. (2024). “Improving Assessment of'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content=\"Tutoring Practices using Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.14594. \\n[20] Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A., & Jamal, A. (2024). “Enhancing Textbook Question Answering Task with \\nLarge Language Models and Retrieval Augmented Generation,” arXiv preprint arXiv:2402.05128. \\n[21] Bucur, M. (2023). “Exploring Large Language Models and Retrieval Augmented Generation for Automated Form Filling,” (Bachelor's \\nthesis, University of Twente).\"),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='thesis, University of Twente). \\n[22] Zhang, B., Yang, H., Zhou, T., Ali Babar, M., & Liu, X. Y. (2023). “Enhancing financial sentiment analysis via retrieval augmented large \\nlanguage models,” In Proceedings of the Fourth ACM International Conference on AI in Finance: 349-356. \\n[23] Al Ghadban, Y., Lu, H. Y., Adavi, U., Sharma, A., Gara, S., Das, N., ... & Hirst, J. E. (2023). “Transforming healthcare education:'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='Harnessing large language models for frontline health worker capacity building using retrieval-augmented generation,” medRxiv, 2023-12. \\n[24] Jeong, M., Sohn, J., Sung, M., & Kang, J. (2024). “Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-\\nAugmented Large Language Models,” arXiv preprint arXiv:2401.15269. \\n[25] Xia, M., Zhang, X., Couturier, C., Zheng, G., Rajmohan, S., & Ruhle, V. (2023). “Hybrid retrieval-augmented generation for real-time'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='composition assistance,” arXiv preprint arXiv:2308.04215. \\n[26] Rackauckas, Z. (2024). “RAG-Fusion: A New Take on Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.03367. \\n[27] Shi, E., Wang, Y., Tao, W., Du, L., Zhang, H., Han, S., ... & Sun, H. (2022). “RACE: Retrieval-Augmented Commit Message \\nGeneration,” arXiv preprint arXiv:2203.02700. \\n[28] Colverd, G., Darm, P., Silverberg, L., & Kasmanoff, N. (2023). “FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='Generation with an LLM,” arXiv preprint arXiv:2311.02597. \\n[29] Huang, W., Lapata, M., Vougiouklis, P., Papasarantopoulos, N., & Pan, J. (2023). “Retrieval Augmented Generation with Rich Answer \\nEncoding,” In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-\\nPacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers): 1012-1025.'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='[30] Chen, W., Hu, H., Saharia, C., & Cohen, W. W. (2022). “Re-imagen: Retrieval-augmented text-to-image generator,” arXiv preprint \\narXiv:2209.14491. \\n[31] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S. W., & Svyatkovskiy, A. (2022). “Reacc: A retrieval-augmented code completion \\nframework,” arXiv preprint arXiv:2203.07722. \\n[32] Wen, Z., Tian, Z., Wu, W., Yang, Y., Shi, Y., Huang, Z., & Li, D. (2023). “Grove: a retrieval-augmented complex story generation'),\n",
              " Document(metadata={'page': 8, 'type': 'text'}, page_content='framework with a forest of evidence,” arXiv preprint arXiv:2310.05388.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='3790\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n[33] Li, S., Park, S., Lee, I., & Bastani, O. (2023). “TRAC: Trustworthy Retrieval Augmented Chatbot,” arXiv preprint arXiv:2307.04642. \\n[34] Lozano, A., Fleming, S. L., Chiang, C. C., & Shah, N. (2023). “Clinfo. ai: An open-source retrieval-augmented large language model system \\nfor answering medical questions using scientific literature,” In Pacific symposium on Biocomputing 2024: 8-23.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='[35] Ding, W., Cao, Y., Zhao, D., Xiao, C., & Pavone, M. (2023). “RealGen: Retrieval Augmented Generation for Controllable Traffic \\nScenarios,” arXiv preprint arXiv:2312.13303. \\n[36] Thompson, W. E., Vidmar, D. M., De Freitas, J. K., Pfeifer, J. M., Fornwalt, B. K., Chen, R., ... & Miotto, R. (2023). “Large Language \\nModels with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,” arXiv preprint arXiv:2312.06457.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='[37] Wang, W., Wang, Y., Joty, S., & Hoi, S. C. (2023). “Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program \\nrepair,” In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software \\nEngineering: 146-158. \\n[38] Guo, Y., Li, Z., Jin, X., Liu, Y., Zeng, Y., Liu, W., ... & Cheng, X. (2023). “Retrieval-augmented code generation for universal information \\nextraction,” arXiv preprint arXiv:2311.02962.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='extraction,” arXiv preprint arXiv:2311.02962. \\n[39] Kagaya, T., Yuan, T. J., Lou, Y., Karlekar, J., Pranata, S., Kinose, A., ... & You, Y. (2024). “RAP: Retrieval-Augmented Planning with \\nContextual Memory for Multimodal LLM Agents,” arXiv preprint arXiv:2402.03610. \\n[40] Fan, R. Z., Fan, Y., Chen, J., Guo, J., Zhang, R., & Cheng, X. (2023). “RIGHT: Retrieval-augmented Generation for Mainstream Hashtag \\nRecommendation,” arXiv preprint arXiv:2312.10466.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='Recommendation,” arXiv preprint arXiv:2312.10466. \\n[41] Jiang, S., Tang, W., Chen, X., Tanga, R., Wang, H., & Wang, W. (2023). Raucg: Retrieval-augmented unsupervised counter narrative \\ngeneration for hate speech. arXiv preprint arXiv:2310.05650. \\n[42] Xu, R., Yu, Y., Ho, J., & Yang, C. (2023). “Weakly-supervised scientific document classification via retrieval-augmented multi-stage'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='training,” In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval: 2501-\\n2505. \\n[43] Hu, M., Zhao, X., Wei, J., Wu, J., Sun, X., Li, Z., ... & Zhang, Y. (2023). “rT5: A Retrieval-Augmented Pre-trained Model for Ancient \\nChinese Entity Description Generation,” In International Conference on NLP and Chinese Computing. Cham: Springer: 736-748.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='[44] Song, S., Lv, Q., Geng, L., Cao, Z., & Fu, G. (2023). “RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling \\nCheck,” In CCF International Conference on Natural Language Processing and Chinese Computing. Cham: Springer: 551-562.  \\n[45] Shi, P., Zhang, R., Bai, H., & Lin, J. (2022). “Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql \\nsemantic parsing,” arXiv preprint arXiv:2210.13693.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='semantic parsing,” arXiv preprint arXiv:2210.13693. \\n[46] Asai, A., Wu, Z., Wang, Y., Sil, A., & Hajishirzi, H. (2023). “Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection,” arXiv preprint arXiv:2310.11511. \\n[47] Lin, D. (2024). “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,” arXiv preprint \\narXiv:2401.12599.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='arXiv:2401.12599. \\n[48] He, X., Tian, Y., Sun, Y., Chawla, N. V., Laurent, T., LeCun, Y., ... & Hooi, B. (2024). “G-Retriever: Retrieval-Augmented Generation for \\nTextual Graph Understanding and Question Answering,” arXiv preprint arXiv:2402.07630. \\n[49] Ahmad, S. R. (2024). “Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model \\nImplementation for Multicultural Enterprise,” arXiv preprint arXiv:2401.01511.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='Implementation for Multicultural Enterprise,” arXiv preprint arXiv:2401.01511. \\n[50] Zhao, C., Jiang, Y., Qiu, Y., Zhang, H., & Yang, W. Y. (2023). “Differentiable Retrieval Augmentation via Generative Language Modeling \\nfor E-commerce Query Intent Classification,” In Proceedings of the 32nd ACM International Conference on Information and Knowledge \\nManagement: 4445-4449.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='Management: 4445-4449. \\n[51] Nakhod, o. Using retrieval-augmented generation to elevate low-code developer skills. https://doi.org/10.15407/jai2023.03.126 \\n[52] Wang, H., Huang, W., Deng, Y., Wang, R., Wang, Z., Wang, Y., ... & Wong, K. F. (2024). “UniMS-RAG: A Unified Multi-source \\nRetrieval-Augmented Generation for Personalized Dialogue Systems,” arXiv preprint arXiv:2401.13256.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='[53] Du, X., & Ji, H. (2022). “Retrieval-augmented generative question answering for event argument extraction,” arXiv preprint \\narXiv:2211.07067. \\n[54] Ranade, P., & Joshi, A. (2023). “FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction,” arXiv \\npreprint arXiv:2310.13848. \\n[55] Zhang, Z., Fang, M., & Chen, L. (2024). “RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='Question Answering,” arXiv preprint arXiv:2402.16457. \\n[56] Zhang, S., Yadav, D., & Jin, T. (2023). “Cash transaction booking via retrieval augmented LLM. KDD 2023 Workshop on Robust NLP for \\nFinance (RobustFin),” https://www.amazon.science/publications/cash-transaction-booking-via-retrieval-augmented-llm \\n[57] Pouplin, T., Sun, H., Holt, S., & Van der Schaar, M. (2024). “Retrieval-Augmented Thought Process as Sequential Decision Making,” arXiv \\npreprint arXiv:2402.07812.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='preprint arXiv:2402.07812. \\n[58] Munikoti, S., Acharya, A., Wagle, S., & Horawalavithana, S. (2023). “ATLANTIC: Structure-Aware Retrieval-Augmented Language Model \\nfor Interdisciplinary Science,” arXiv preprint arXiv:2311.12289. \\n[59] Markey, N., El-Mansouri, I., Rensonnet, G., van Langen, C., & Meier, C. (2024). “From RAGs to riches: Using large language models to \\nwrite documents for clinical trials,” arXiv preprint arXiv:2402.16406.'),\n",
              " Document(metadata={'page': 9, 'type': 'text'}, page_content='write documents for clinical trials,” arXiv preprint arXiv:2402.16406. \\n[60] Kim, J., & Min, M. (2024). “From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process,” arXiv \\npreprint arXiv:2402.01717.')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unified FAISS vector store with CLIP embeddings\n",
        "embeddings_array = np.array(all_embeddings)\n",
        "embeddings_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNiL1o_4OUbA",
        "outputId": "720a7804-7b67-485c-f866-03b88b84c6ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.03966911,  0.02233979,  0.03567739, ..., -0.02313417,\n",
              "        -0.04350961, -0.0356045 ],\n",
              "       [-0.00800639, -0.01203629, -0.00161855, ...,  0.00660626,\n",
              "         0.00440606, -0.06144243],\n",
              "       [ 0.00099998,  0.01867807, -0.02668937, ..., -0.09358583,\n",
              "        -0.03183691,  0.03085243],\n",
              "       ...,\n",
              "       [ 0.07239746, -0.04516119, -0.02439865, ..., -0.04424017,\n",
              "        -0.04348588, -0.01945706],\n",
              "       [-0.00251454, -0.04437987, -0.01670402, ..., -0.03156574,\n",
              "        -0.08298665, -0.03596834],\n",
              "       [-0.0059841 , -0.00655963, -0.03402936, ..., -0.03022855,\n",
              "        -0.0441472 , -0.02386659]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(all_docs,embeddings_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3jKHL8bOV0_",
        "outputId": "e9aadde0-1f18-4ad1-9c09-b5cb8fe36155"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Document(metadata={'page': 0, 'type': 'text'}, page_content='ScienceDirect\\nAvailable online at www.sciencedirect.com\\nProcedia Computer Science 246 (2024) 3781–3790\\n1877-0509 © 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge \\nBased and Intelligent information and Engineering Systems\\n10.1016/j.procs.2024.09.178'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='Based and Intelligent information and Engineering Systems\\n10.1016/j.procs.2024.09.178\\nKeywords: Large Language Models (LLMs); Natural Language Processing (NLP); Retrieval-Augmented Generation (RAG); Text generation; \\nDigital transformation. \\n1. Introduction \\nDigital transformation signifies the incorporation of digital technology across different facets of a business, \\nreshaping its operations and value delivery to customers [1]. At the forefront of driving such transformative'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='practices are Large Language Models (LLMs), advanced machine learning models trained extensively on textual \\ndata to comprehend and produce human-like text [1]. LLMs, such as the Generative Pre-training Transformer (GPT) \\n \\n \\n* Corresponding author. Tel.: +33 03 80 39 50 00; fax: +33 03 80 39 50 69.  \\nE-mail address: muhammad.arslan@u-bourgogne.fr \\n28th International Conference on Knowledge-Based and Intelligent Information & Engineering \\nSystems (KES 2024) \\nA Survey on RAG with LLMs'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='Systems (KES 2024) \\nA Survey on RAG with LLMs \\nMuhammad Arslana*, Hussam Ghanema, Saba Munawarb and Christophe Cruza \\naLaboratoire Interdisciplinaire Carnot de Bourgogne (ICB), Dijon, France \\nbNational University of Computer and Emerging Sciences (NUCES), Islamabad, Pakistan                                                                 \\nAbstract \\nIn the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='competitive and foster growth. Large Language Models (LLMs) have emerged as game-changers across industries, \\nrevolutionizing various sectors by harnessing extensive text data to analyze and generate human-like text. Despite their \\nimpressive capabilities, LLMs often encounter challenges when dealing with domain-specific queries, potentially leading to \\ninaccuracies in their outputs. In response, Retrieval-Augmented Generation (RAG) has emerged as a viable solution. By'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='seamlessly integrating external data retrieval into text generation processes, RAG aims to enhance the accuracy and relevance of \\nthe generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG, \\noverlooking a comprehensive exploration of its applications. This paper seeks to address this gap by providing a thorough review'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='of RAG applications, encompassing both task-specific and discipline-specific studies, while also outlining potential avenues for \\nfuture research. By shedding light on current RAG research and outlining future directions, this review aims to catalyze further \\nexploration and development in this dynamic field, thereby contributing to ongoing digital transformation efforts. \\n© 2024 The Authors. Published by Elsevier B.V.'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='© 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge Based and \\nIntelligent information and Engineering Systems'),\n",
              "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]'),\n",
              "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_1'}, page_content='[Image: page_0_img_1]'),\n",
              "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_2'}, page_content='[Image: page_0_img_2]'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='3782\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nseries [2, 3] and others, have demonstrated remarkable capabilities in NLP tasks [4]. However, these models face \\nchallenges when dealing with domain-specific queries, often generating inaccurate or irrelevant information, \\ncommonly referred to as “hallucinations”, particularly when data is sparse [5]. This limitation makes deploying'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='LLMs in real-world settings impractical, as the generated output may not be reliable [4].  \\nIn the middle of 2020, Lewis et al. [6] introduced RAG, a significant advancement in the field of LLMs for \\nimproving generative tasks (see Fig. 1 (a)). RAG incorporates an initial step where LLMs search an external data \\nsource to retrieve relevant information before producing text or answering questions. RAG addresses these'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='limitations by integrating external data retrieval into the generative process, thereby enhancing the accuracy and \\nrelevance of the generated output. By dynamically retrieving information from knowledge bases during inference, \\nRAG provides a more informed and evidence-based approach to language generation, significantly reducing the risk \\nof hallucinations and improving the overall quality of the generated text [4, 6]. This approach has the potential to'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='make LLMs more practical for real-world applications, as it ensures that the generated output is grounded in \\nretrieved evidence, leading to more reliable and accurate results. Fig. 1 (b) showcases how real-time business \\nsystems can leverage the RAG with LLM architecture. As an example, without RAG, the system lacks access to \\nreal-time or updated information. However, with RAG integration, leveraging external data sources such as news'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='articles, the system can respond to current business events, presenting opportunities for business intelligence \\nanalysts. \\n \\n                                  (a)                                                                                             (b) \\nFig. 1. (a) A generic RAG architecture, where users’ queries, potentially in different modalities (e.g., text, code, image, etc.), are inputted into'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='both the retriever and the generator. The retriever scans for relevant data sources in storage, while the generator engages with the retrieval \\noutcomes, ultimately generating results across various modalities [6]; Fig. 1. (b) illustrates how RAG integration with the LLM handles queries \\nthat fall outside the scope of the LLM’s training data.  \\nWhile the field of RAG has seen substantial growth, several online surveys [4, 7, 8, 9] have explored'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='technological advancements in RAG. Although these surveys provide valuable insights and references, they offer \\nonly a limited overview of RAG applications. To address this gap, this paper aims to provide an exhaustive \\noverview of RAG applications, including both task-specific and discipline-specific studies, as well as future \\ndirections. By highlighting the current state of RAG research and its potential future directions, this review aims to'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='inspire further investigation and development in this exciting field. \\nThe paper’s structure is as follows: Section 2 presents the adopted research methodology for this survey. In \\nSection 3, we provide an overview of RAG applications, followed by a detailed discussion in Section 4. The paper \\nconcludes in Section 5, summarizing the key findings and implications of the study. \\n2. Background'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='concludes in Section 5, summarizing the key findings and implications of the study. \\n2. Background \\nThe research method (see Fig. 2) employed in this paper involves a thorough review and analysis of research \\npublications related to RAG. The main objective is to identify and categorize its applications across various NLP \\ntasks and disciplines. The paper begins by collecting research publications specific to RAG, focusing on their'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='applications. Since the RAG with LLM domain is relatively new and emerging, with many studies available as pre-'),\n",
              "  Document(metadata={'page': 1, 'type': 'image', 'image_id': 'page_1_img_0'}, page_content='[Image: page_1_img_0]'),\n",
              "  Document(metadata={'page': 1, 'type': 'image', 'image_id': 'page_1_img_1'}, page_content='[Image: page_1_img_1]'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3783\\nprints online, limiting the search to platforms such as Scopus or IEEE would greatly reduce the number of studies. \\nTherefore, Google Scholar was utilized to access the studies on RAG. However, in cases where both pre-print and \\npublished versions of a study were available, the published version was chosen to cover the maximum number of'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='peer-reviewed studies. Each study underwent manual review to assess its comprehensiveness and depth, excluding \\nshort studies. It is important to note that the purpose of the survey is not to cover the most optimal studies, but rather \\nto provide an overview of how this field has attained significant attention in a short period, with researchers \\nexploring diverse application scenarios.  \\nThe keywords used to collect research publications included “retrieval augmented generation”, “RAG'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='The keywords used to collect research publications included “retrieval augmented generation”, “RAG \\napplications”, “generative models with retrieval”, “external data retrieval in text generation”, “enhancing text \\ngeneration with retrieval”, “integrating retrieval into generative models”, “external knowledge in text generation”, \\n“retrieval-based text generation”, “information retrieval for text generation”, and “contextualized retrieval in'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='language models”. These publications are then classified into two principal categories: task-based classification and \\ndiscipline-based classification. Task-based classification focuses on categorizing RAG studies according to their \\nexecution of information processing tasks, particularly within NLP. Conversely, discipline-based classification \\ncategorizes studies based on their application to specific domains. Under the task-based classification, the'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='publications are further subdivided into categories such as Question Answering (QA), Text Generation and \\nSummarization, Information Retrieval and Extraction, Text Analysis and Processing, Software Development and \\nMaintenance (SDM), Decision Making and Applications, and Other Categories. Similarly, under the discipline-\\nbased classification, the publications are further subdivided into categories such as Medical/Biomedical, Financial,'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='Educational, Technology and Software Development, Social and Communication, Literature, and Other Categories. \\nThese categories are selected based on an understanding of the context of the studies and the underlying problems \\nthey address. Within both classification methods, “software development” stands out as a common category. It \\ninvolves programming information processing tasks under task-based classification and encompasses systems for'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='developing various applications across different domains under discipline-based classification. Figure 3 illustrates \\nthe number of publications related to RAG applications from 2020 to February 2024. Specifically, there was a single \\npublication found in 2020, 6 publications in 2022, 28 publications in 2023, and 16 publications until February 2024, \\nindicating a growing interest and research activity in the field of RAG applications. \\nFig. 2. Research Method'),\n",
              "  Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_0'}, page_content='[Image: page_2_img_0]'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='3784\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Evolution of Research Publications on RAG Applications \\n3. Applications of RAG with LLMs \\nUpon thorough examination of the selected papers focusing on RAG applications, we uncovered a vast array of \\ndiverse applications. These findings are distilled into a comprehensive table format (see Table 1), detailing three'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='crucial aspects: 1) Use case with RAG, 2) Used datasets/benchmarks, and 3) Application area. Noteworthy \\napplications span various domains, including biomedical, financial, and medical inquiries, alongside text \\nsummarization and book review generation. RAG’s versatility extends to commonsense QA, table-based queries, \\nand clinical decision-making, among others. It further encompasses educational decision making, textbook question'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='answering, and enterprise search functionalities. RAG is instrumental in sentiments classification, health education, \\nand generating biomedical explanations, while also enhancing user writing accuracy and speed. Its utility spans \\nhumanitarian assistance, generating informative dialogues, crafting realistic images and intricate plotlines, and much \\nmore.  \\nAdditionally, RAG aids in natural language QA, disease identification, and information extraction. It handles'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='decision-making tasks, hashtag management, hate speech detection, and scientific document classification. RAG \\nexcels in entity description generation, text correction, and SQL translation, while also enhancing open-domain QA \\nand professional knowledge inquiries. Also, it extends the capabilities of machine translation tasks beyond text-to-\\nSQL, such as neural text re-ranking [6]. Moreover, it supports multicultural enterprise queries, e-commerce'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='searches, and personalized dialogue systems. Furthermore, RAG facilitates event argument extraction, intelligence \\nreport generation, short-form QA, automated transactions, and private data handling. Lastly, it contributes to science \\nQA, clinical writing, and pharmaceutical regulatory compliance inquiries. \\nAfter compiling all the applications of RAG, the subsequent step involves categorizing them based on the'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='specific nature of the NLP tasks they tackle (see Table 2 and Fig. 4). From the compiled publications, it was \\nobserved that 20 studies were dedicated to QA, 6 to Text Generation and Summarization, 6 to Information Retrieval \\nand Extraction, 5 to Text Analysis and Processing, 4 to SDM, and 5 to Decision Making and Applications, while the \\nremaining 6 studies were classified under \"Other Categories.\" This classification is significant as it helps in'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='understanding the distribution and focus of RAG applications across different NLP tasks. Additionally, since RAG \\napplications span various disciplines, further classification (see Table 3 and Fig. 5) reveals that 9 publications were \\nrelated to Medical/Biomedical, 2 to Financial, 2 to Educational, 9 to Technology and Software Development, 7 to \\nSocial and Communication, and 3 to Literature, with the remaining falling into \"Other Categories\". \\nTable 1. Applications of RAG  \\nNo.'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='Table 1. Applications of RAG  \\nNo. \\nUse case with RAG \\nUsed datasets / benchmarks \\nApplication area \\n1 \\nMIRAGE: Medical information RAG [10]  \\nMedical QA datasets \\nBiomedical QA \\n2 \\nRAG for improved context accuracy [11]  \\nFinancial reports \\nFinancial QA \\n3 \\nRetrieval-augmented Electrocardiography (ECG) [12]  \\nCardiac symptoms and sleep apnea diagnosis \\nMedical QA \\n4 \\nRepresentative Vector Summarization (RVS) [13]  \\nPDFs, text documents, spreadsheets, etc.  \\nMedical text summarization \\n5'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='PDFs, text documents, spreadsheets, etc.  \\nMedical text summarization \\n5 \\nRetrieval-augmented controllable reviews [14] \\nAmazon book reviews \\nBook review generation'),\n",
              "  Document(metadata={'page': 3, 'type': 'image', 'image_id': 'page_3_img_0'}, page_content='[Image: page_3_img_0]'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3785\\n6 \\nRetrieval-augmented knowledge graph reasoning [15] \\nCommonsense QA and OpenBookQA. \\nCommonsense QA \\n7 \\nAnswers from table corpus via RAG [16] \\nWikipedia data \\nTable QA  \\n8 \\nLiVersa: a liver disease specific LLM using RAG [17] \\nLiver Diseases  \\nMedical QA \\n9 \\nAlmanac: RAG for clinical medicine [18] \\nGuidelines and treatment recommendations. \\nClinical decision-making \\n10 \\nAssessment of tutoring practices [19]'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Clinical decision-making \\n10 \\nAssessment of tutoring practices [19] \\nDialogue transcripts from a middle-school.  \\nEducational decision making \\n11 \\nHandling out of domain scenarios [20] \\nLife science, earth science, etc. lessons.  \\nTextbook QA \\n12 \\nAutomated form filling [21] \\nRequest forms for IT projects \\nEnterprise search \\n13 \\nFinancial sentiment analysis [22]  \\nTwitter financial news and FiQA datasets \\nSentiments classification \\n14 \\nFrontline health worker capacity building [23]'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Sentiments classification \\n14 \\nFrontline health worker capacity building [23] \\nPregnancy-related guidelines \\nHealth education QA \\n15 \\nSelf-BioRAG: a framework for biomedical text [24] \\nBiomedical instruction sets \\nBiomedical Informatics \\n16 \\nHybrid RAG for real-time composition assistance [25] \\nWikiText-103, Enron Emails, etc.  \\nWriting speed and accuracy \\n17 \\nRAG-Fusion to obtain product information [26] \\nProduct datasheets \\nTechnical information QA \\n18'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='RAG-Fusion to obtain product information [26] \\nProduct datasheets \\nTechnical information QA \\n18 \\nCommit message generation for code intelligence [27]  \\nMCMD dataset  \\nSDM \\n19 \\nFloodBrain: Flood disaster reporting [28] \\nReliefWeb reports \\nHumanitarian assistance \\n20 \\nRich answer encoding [29] \\nMSMARCO QA and WoW dataset. \\nGenerative QA  \\n21 \\nText-to-image generator [30] \\nCOCO and WikiImages datasets. \\nRealistic images generation \\n22 \\nCode completion framework [31]'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='COCO and WikiImages datasets. \\nRealistic images generation \\n22 \\nCode completion framework [31] \\nCodeXGLUE and CodeNet datasets. \\nSDM \\n23 \\nComplex story generation framework [32] \\nIMDB movie details dataset \\nGenerate stories  \\n24 \\nTRAC: Trustworthy retrieval augmented chatbot [33] \\nNatural Question dataset \\nNatural QA \\n25 \\nClinfo.ai using scientific literature [34] \\nPubMed dataset \\nMedical QA \\n26 \\nRealGen for controllable traffic scenarios [35] \\nnuScenes dataset \\nCritical traffic scenarios \\n27'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='RealGen for controllable traffic scenarios [35] \\nnuScenes dataset \\nCritical traffic scenarios \\n27 \\nZero-shot disease phenotyping [36]  \\nClinical notes \\nIdentifying diseases \\n28 \\nRAP-Gen for automatic program repair [37]  \\nTFix, Defects4J, etc. datasets \\nSDM \\n29 \\nCode4UIE : retrieval-augmented code generation [38] \\nACE04, ACE05, CoNLL03, etc. datasets \\nInformation extraction  \\n30 \\nRAP: retrieval-augmented planning [39] \\nALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='30 \\nRAP: retrieval-augmented planning [39] \\nALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31 \\nRIGHT for mainstream hashtag recommendation [40] \\nTwitter and Weibo data.  \\nRetrieval-enhanced hashtags \\n32 \\nRAUCG for counter narrative generation for hate speech \\n[41] \\nMultitargetCONAN dataset \\nCombating hate speech \\n33 \\nWeakly-supervised scientific document classification \\n[42] \\nAGNews and MeSH datasets.  \\nScientific documents \\nclassification \\n34'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='[42] \\nAGNews and MeSH datasets.  \\nScientific documents \\nclassification \\n34 \\nrT5 for Chinese entity description generation [43] \\nXunZi and MengZi datasets.  \\nEntity description generation \\n35 \\nRSpell: domain adaptive Chinese spelling check [44]  \\nCSC dataset \\nText error correction \\n36 \\nXRICL: cross-lingual retrieval-augmented in-context \\nlearning for cross-lingual text-to-SQL semantic parsing \\n[45] \\nXSPIDER and XKAGGLE-DBQA datasets. \\nText-to-SQL translation \\n37'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='[45] \\nXSPIDER and XKAGGLE-DBQA datasets. \\nText-to-SQL translation \\n37 \\nSELF-RAG: learning to retrieve, generate, and \\ncritique through self-reflection [46] \\nOpen-Instruct processed data. \\nOpen-domain QA and fact \\nverification \\n38 \\nChatDOC with enhanced PDF structure recognition [47] \\nAcademic papers, financial reports, \\ntextbooks, and legislative materials \\nProfessional knowledge QA \\n39 \\nG-Retriever for textual graph understanding [48] \\nGraphQA (ExplaGraphs, SceneGraphs and \\nWebQSP)'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='G-Retriever for textual graph understanding [48] \\nGraphQA (ExplaGraphs, SceneGraphs and \\nWebQSP)  \\nChat with graphs \\n40 \\nEnhancing multilingual information retrieval in \\nmixed Human Resources (HR) environments [49] \\nHR standard operating procedures and \\nQuality Assurance (QA) documents \\nMulticultural enterprise QA \\n41 \\nDifferentiable RAG [50] \\nUser-clicked logs \\nE-commerce search (query \\nintent classification) \\n42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data \\nSDM \\n43 \\nUniMS-RAG: a unified multi-source RAG [52] \\nDuLeMon and KBP datasets \\nPersonalized dialogue \\nsystems \\n44 \\nRAG QA for event argument extraction [53] \\nACE 2005 and WikiEvent datasets \\nEvent argument (answer) \\nextraction \\n45 \\nFABULA: retrieval-augmented narrative construction \\n[54] \\nOntoNotes and Pile datasets \\nIntelligence report \\ngeneration \\n46 \\nTime-Aware Adaptive Retrieval (TA-ARE) [55]'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Intelligence report \\ngeneration \\n46 \\nTime-Aware Adaptive Retrieval (TA-ARE) [55] \\nRetrievalQA dataset \\nShort-form open-domain \\nQA \\n47 \\nCash transaction booking via RAG [56] \\nCash Management Software (CMS) \\ntransactions. \\nAutomated cash transaction \\nbooking \\n48 \\nRetrieval-Augmented Thought Process (RATP) [57] \\nBoolq and emrQA datasets. \\nQA with private data \\n49 \\nATLANTIC for interdisciplinary science [58] \\nS2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='S2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50 \\nWriting documents for clinical trials [59] \\nFDA guidance database, ClinicalTrials.gov, \\nand AACT database.  \\nClinical-related writing \\n51 \\nQA RAG model [60] \\nFDA Q&A datasets  \\nPharma industry regulatory \\ncompliance QA'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='3786\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n4. Discussion  \\nThe classification of RAG applications according to the specific NLP tasks they target holds significant \\nimportance for several reasons. Firstly, it offers valuable insights into the distribution and focus of RAG applications \\nacross various tasks within the field of NLP. By quantifying the number of studies dedicated to each task,'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='researchers gain a deeper understanding of where efforts and resources are predominantly concentrated within the \\nRAG domain. By analyzing the distribution of RAG applications, researchers can discern prevailing trends in \\nresearch interest and identify emerging areas of importance. The classification of RAG applications based on \\ndiscipline offers valuable insights into its widespread adoption across various domains. This classification not only'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='provides a comprehensive understanding of RAG’s applicability but also underscores its potential to revolutionize \\nvarious domains, thereby contributing significantly to the advancement of NLP technologies.  \\nWhile this survey offers a comprehensive overview of RAG applications across various NLP tasks and \\ndisciplines, it also has its limitations. 1) Given that RAG technology is still emerging, the majority of RAG-based'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='studies are available in pre-print formats on platforms like arXiv, lacking peer review. This raises questions about \\ntheir authenticity. 2) Additionally, the survey overlooks the technical implementation details and challenges \\nassociated with using RAG technology alongside open-source LLMs. Organizations may find RAG implementation \\ncostly if they do not opt for open-source LLM architectures, especially considering the expense of querying the'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='LLM via Application Programming Interface (API). 3) Furthermore, the performance of RAG concerning the \\nvolume and variety of datasets has not been discussed. Deploying RAG with large datasets of varying structures \\n(e.g., structured, semi-structured, or non-structured) may lead to processing delays, warranting further exploration \\nbefore selecting a RAG with LLM integrated solution for organizational deployment.'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='before selecting a RAG with LLM integrated solution for organizational deployment.  \\n4) Additionally, this survey did not cover the diverse range of RAG architectures and technologies available for \\nintegration with different LLMs. Future work should delve into these options to discuss how various RAG solutions \\ncan be adapted with LLMs for different NLP tasks and applications. 5) Furthermore, the survey did not address the'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='accuracy of information obtained from RAG with LLM solutions. It is essential to explore the reliability of these \\nsystems and assess the organizations’ dependency on their generated responses. LLMs often generate responses with \\nhigh confidence, making it challenging to evaluate the accuracy of the information provided. 6) While the survey \\nprimarily focuses on task-based and discipline-based applications of RAG, there is a need for further research to'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='explore ethical considerations associated with its usage, especially when dealing with sensitive datasets. For \\nexample, in the biomedical domain, RAG has the potential to accidentally expose private information to analysts, \\nraising concerns about data privacy and security. Additionally, in the legal domain, RAG may mistakeably reveal \\nprivileged information during document analysis, potentially violating client confidentiality and attorney-client'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='privilege. Therefore, future studies should delve deeper into these ethical implications to ensure responsible and \\nethical use of RAG technology across various domains. \\n5. Conclusion  \\nThis article offers a thorough examination of the applications of RAG with LLMs, showcasing their potential to \\ndrive digital transformation across diverse industries. Initially, it gathers the latest publications on RAG from online'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='repositories. These publications are then classified based on task-oriented and discipline-oriented criteria. A notable \\ntrend observed is the increasing number of research papers on RAG deposited in open-access sources, particularly \\nsince 2023. However, many works remain unpublished or are in the preprint stage, awaiting review by various \\njournals. A significant portion of these studies primarily focus on the task of QA in NLP. Conversely, there is a'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='noticeable gap in research exploring Entity Linking, an essential NLP task that contributes to knowledge graph \\ndevelopment. Addressing this gap could unlock numerous applications in the realm of linked data. Regarding \\ndisciplines, the majority of research applications are concentrated in the fields of Medical/Biomedical and \\nTechnology and Software Development. In contrast, disciplines such as Business and Agriculture receive'),\n",
              "  Document(metadata={'page': 5, 'type': 'text'}, page_content='comparatively less attention. Future research endeavors should aim to bridge this gap by addressing the specific \\nneeds of these underrepresented disciplines.'),\n",
              "  Document(metadata={'page': 6, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3787\\nTable 2. Task-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 4. Task-based classification of RAG applications with count of publications. The word cloud is generated based on the publication counts'),\n",
              "  Document(metadata={'page': 6, 'type': 'text'}, page_content='listed under various headings in Table 2. \\n \\n \\n \\n \\n1) Question Answering (QA)  \\n- Biomedical QA [1] \\n- Financial QA [2] \\n- Medical QA [3] \\n- Commonsense QA [6] \\n- Textbook QA [11] \\n- Health education QA [14] \\n- Technical product information QA [17] \\n- Natural QA [24] \\n- Professional knowledge QA [38] \\n- Multicultural enterprise QA [40] \\n- Open-domain QA and fact verification [46] \\n- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29]'),\n",
              "  Document(metadata={'page': 6, 'type': 'text'}, page_content='- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29] \\n- Pharma industry regulatory compliance QA [51] \\n- Science QA and document classification [49] \\n- Clinical-related writing [50] \\n- Personalized dialogue systems [43] \\n2) Text Generation and Summarization \\n- Medical text summarization [4] \\n- Book review generation [5] \\n- Biomedical Informatics [15] \\n- Generate stories with complex plots [23] \\n- Generate realistic and faithful images [21]'),\n",
              "  Document(metadata={'page': 6, 'type': 'text'}, page_content='- Generate stories with complex plots [23] \\n- Generate realistic and faithful images [21] \\n- Entity description generation [34] \\n \\n3) Information Retrieval and Extraction \\n- Table QA [7] \\n- Enterprise search [12] \\n- Retrieval-enhanced hashtags [31] \\n- Information extraction [29] \\n- Event argument (answer) extraction [44] \\n- E-commerce search (query intent classification) [41] \\n4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35]'),\n",
              "  Document(metadata={'page': 6, 'type': 'text'}, page_content='4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Scientific documents classification [33] \\n- Combating online hate speech [32] \\n \\n5) Software Development and \\nMaintenance \\n- Code intelligence [18] \\n- Code completion [22] \\n- Automatic program repair [28] \\n- Elevate low-code developer skills [42] \\n \\n6) Decision Making and Applications \\n- Clinical decision-making [9] \\n- Educational decision making [10]'),\n",
              "  Document(metadata={'page': 6, 'type': 'text'}, page_content='- Clinical decision-making [9] \\n- Educational decision making [10] \\n- Decision-making applications [30] \\n- Automated cash transaction booking [47] \\n- Intelligence report generation [45] \\n \\n7) Other Categories: \\n- Editing and crafting diverse behaviors, \\nincluding critical traffic scenarios [26] \\n- Identifying diseases [27] \\n- Chat with graphs [39] \\nTask: (Count of Publications)  \\nQuestion Answering (QA): (20) \\nText Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6)'),\n",
              "  Document(metadata={'page': 6, 'type': 'text'}, page_content='Text Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6) \\nText Analysis and Processing: (5) \\nSoftware Development and Maintenance: (4) \\nDecision Making and Applications: (5) \\nOther Categories: (6)'),\n",
              "  Document(metadata={'page': 6, 'type': 'image', 'image_id': 'page_6_img_0'}, page_content='[Image: page_6_img_0]'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='3788\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nTable 3. Discipline-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 5. Discipline-based classification of RAG applications with count of publications. The word cloud is generated based on the publication'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='counts listed under various headings in Table 3. \\nAcknowledgements \\nThe authors thank the French Government and the National Research Agency (ANR) for their funding. \\nReferences \\n[1] Roumeliotis KI, Tselikas ND, & Nasiopoulos DK. (2024). “LLMs in e-commerce: a comparative analysis of GPT and LLaMA models in \\nproduct review evaluation,” Natural Language Processing Journal:1-6:100056.'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='product review evaluation,” Natural Language Processing Journal:1-6:100056. \\n[2] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). “Language Models are Few-Shot \\nLearners,” Advances in Neural Information Processing Systems 33 (NeurIPS 2020).  \\n[3] OpenAI, R. (2023). “Gpt-4 technical report,” arxiv 2303.08774. View in Article: 2(5). \\n1) Medical / Biomedical \\n- Biomedical QA [1] \\n- Medical QA [3] \\n- Medical text summarization [4]'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='1) Medical / Biomedical \\n- Biomedical QA [1] \\n- Medical QA [3] \\n- Medical text summarization [4] \\n- Health education QA [14] \\n- Identifying diseases [27] \\n- Clinical decision-making [9] \\n- Clinical-related writing [50] \\n- Science QA and scientific document classification [49] \\n- Pharma industry regulatory compliance QA [51] \\n2) Financial \\n- Financial QA [2] \\n- Automated cash transaction booking [47] \\n3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11]'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11] \\n4) Technology and Software Development \\n- Table QA [7] \\n- Technical product information QA [17] \\n- Software development and maintenance [18, 22, 28, 42] \\n- Generative QA and informative conversations [20] \\n- Information extraction [29] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Personalized dialogue systems [43] \\n- Event argument (answer) extraction [44] \\n5) Social and Communication'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='- Event argument (answer) extraction [44] \\n5) Social and Communication \\n- Commonsense QA [6] \\n- Sentiments classification [13] \\n- Combating online hate speech [32] \\n- Retrieval-enhanced hashtags [31] \\n- Humanitarian assistance [19] \\n- Chat with graphs [39] \\n- Multicultural enterprise QA [40] \\n6) Literature  \\n- Book review generation guided by reference documents [5] \\n- Enhance user writing speed and accuracy [16] \\n- Generate stories with complex plots [23] \\n7) Other Categories'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='- Generate stories with complex plots [23] \\n7) Other Categories   \\n- Enterprise search [12] \\n- Generate realistic and faithful images [21] \\n- Decision-making applications [30] \\n- Open-domain question answering and fact verification [37] \\n- Professional knowledge QA [38] \\n- Intelligence report generation [45] \\n- Short-form open-domain QA [46] \\n- Question answering with private data [48] \\n \\nDiscipline: (Count of Publications)  \\nMedical / Biomedical: (9) \\nFinancial: (2) \\nEducational: (2)'),\n",
              "  Document(metadata={'page': 7, 'type': 'text'}, page_content='Discipline: (Count of Publications)  \\nMedical / Biomedical: (9) \\nFinancial: (2) \\nEducational: (2) \\nTechnology and Software Development: (9) \\nSocial and Communication: (7) \\nLiterature (3) \\nOther Categories: (8)'),\n",
              "  Document(metadata={'page': 7, 'type': 'image', 'image_id': 'page_7_img_0'}, page_content='[Image: page_7_img_0]'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3789\\n[4] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., ... & Wang, H. (2023). “Retrieval-augmented generation for large language models: A \\nsurvey,” arXiv preprint arXiv:2312.10997. \\n[5] Kandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2023). “Large language models struggle to learn long-tail knowledge,” In \\nInternational Conference on Machine Learning. PMLR: 5696-15707.'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='International Conference on Machine Learning. PMLR: 5696-15707. \\n[6] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). “Retrieval-augmented generation for knowledge-\\nintensive nlp tasks,” Advances in Neural Information Processing Systems 33: 9459-9474. \\n[7] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). “A survey on retrieval-augmented text generation,” arXiv preprint arXiv:2202.01110.'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='[8] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., ... & Scialom, T. (2023). “Augmented language models: a \\nsurvey,” arXiv preprint arXiv:2302.07842. \\n[9] Zhao, R., Chen, H., Wang, W., Jiao, F., Do, X. L., Qin, C., ... & Joty, S. (2023). “Retrieving multimodal information for augmented \\ngeneration: A survey,” arXiv preprint arXiv:2303.10868.'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='generation: A survey,” arXiv preprint arXiv:2303.10868. \\n[10] Xiong, G., Jin, Q., Lu, Z., & Zhang, A. (2024). “Benchmarking retrieval-augmented generation for medicine,” arXiv preprint \\narXiv:2402.13178. \\n[11] Jimeno Yepes, A., You, Y., Milczek, J., Laverde, S., & Li, L. (2024). “Financial Report Chunking for Effective Retrieval Augmented \\nGeneration,” arXiv e-prints, arXiv-2402.'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='Generation,” arXiv e-prints, arXiv-2402. \\n[12] Yu, H., Guo, P., & Sano, A. (2023). “Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation,” \\nIn Machine Learning for Health (ML4H) PMLR: 650-663. \\n[13] Manathunga, S. S., & Illangasekara, Y. A. (2023). “Retrieval Augmented Generation and Representative Vector Summarization for large \\nunstructured textual data in Medical Education,” arXiv preprint arXiv:2308.00479.'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='unstructured textual data in Medical Education,” arXiv preprint arXiv:2308.00479. \\n[14] Kim, J., Choi, S., Amplayo, R. K., & Hwang, S. W. (2020). “Retrieval-augmented controllable review generation,” In Proceedings of the \\n28th International Conference on Computational Linguistics: 2284-2295. \\n[15] Sha, Y., Feng, Y., He, M., Liu, S., & Ji, Y. (2023). “Retrieval-augmented Knowledge Graph Reasoning for Commonsense Question'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='Answering,” Mathematics 11(15): 3269; https://doi.org/10.3390/math11153269.  \\n[16] Pan, F., Canim, M., Glass, M., Gliozzo, A., & Hendler, J. (2022). “End-to-End Table Question Answering via Retrieval-Augmented \\nGeneration,” arXiv preprint arXiv:2203.16714. \\n[17] Ge, J., Sun, S., Owens, J., Galvez, V., Gologorskaya, O., Lai, J. C., ... & Lai, K. (2023). “Development of a Liver Disease-Specific Large \\nLanguage Model Chat Interface using Retrieval Augmented Generation,” medRxiv.'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='Language Model Chat Interface using Retrieval Augmented Generation,” medRxiv. \\n[18] Zakka, C., Shad, R., Chaurasia, A., Dalal, A. R., Kim, J. L., Moor, M., ... & Hiesinger, W. (2024). “Almanac—retrieval-augmented language \\nmodels for clinical medicine,” NEJM AI 1(2), AIoa2300068. \\n[19] Han, Z. FeiFei, Lin, J., Gurung, A., Thomas, D. R., Chen, E., Borchers, C., Gupta, S., & Koedinger, K. R. (2024). “Improving Assessment of'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content=\"Tutoring Practices using Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.14594. \\n[20] Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A., & Jamal, A. (2024). “Enhancing Textbook Question Answering Task with \\nLarge Language Models and Retrieval Augmented Generation,” arXiv preprint arXiv:2402.05128. \\n[21] Bucur, M. (2023). “Exploring Large Language Models and Retrieval Augmented Generation for Automated Form Filling,” (Bachelor's \\nthesis, University of Twente).\"),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='thesis, University of Twente). \\n[22] Zhang, B., Yang, H., Zhou, T., Ali Babar, M., & Liu, X. Y. (2023). “Enhancing financial sentiment analysis via retrieval augmented large \\nlanguage models,” In Proceedings of the Fourth ACM International Conference on AI in Finance: 349-356. \\n[23] Al Ghadban, Y., Lu, H. Y., Adavi, U., Sharma, A., Gara, S., Das, N., ... & Hirst, J. E. (2023). “Transforming healthcare education:'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='Harnessing large language models for frontline health worker capacity building using retrieval-augmented generation,” medRxiv, 2023-12. \\n[24] Jeong, M., Sohn, J., Sung, M., & Kang, J. (2024). “Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-\\nAugmented Large Language Models,” arXiv preprint arXiv:2401.15269. \\n[25] Xia, M., Zhang, X., Couturier, C., Zheng, G., Rajmohan, S., & Ruhle, V. (2023). “Hybrid retrieval-augmented generation for real-time'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='composition assistance,” arXiv preprint arXiv:2308.04215. \\n[26] Rackauckas, Z. (2024). “RAG-Fusion: A New Take on Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.03367. \\n[27] Shi, E., Wang, Y., Tao, W., Du, L., Zhang, H., Han, S., ... & Sun, H. (2022). “RACE: Retrieval-Augmented Commit Message \\nGeneration,” arXiv preprint arXiv:2203.02700. \\n[28] Colverd, G., Darm, P., Silverberg, L., & Kasmanoff, N. (2023). “FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='Generation with an LLM,” arXiv preprint arXiv:2311.02597. \\n[29] Huang, W., Lapata, M., Vougiouklis, P., Papasarantopoulos, N., & Pan, J. (2023). “Retrieval Augmented Generation with Rich Answer \\nEncoding,” In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-\\nPacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers): 1012-1025.'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='[30] Chen, W., Hu, H., Saharia, C., & Cohen, W. W. (2022). “Re-imagen: Retrieval-augmented text-to-image generator,” arXiv preprint \\narXiv:2209.14491. \\n[31] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S. W., & Svyatkovskiy, A. (2022). “Reacc: A retrieval-augmented code completion \\nframework,” arXiv preprint arXiv:2203.07722. \\n[32] Wen, Z., Tian, Z., Wu, W., Yang, Y., Shi, Y., Huang, Z., & Li, D. (2023). “Grove: a retrieval-augmented complex story generation'),\n",
              "  Document(metadata={'page': 8, 'type': 'text'}, page_content='framework with a forest of evidence,” arXiv preprint arXiv:2310.05388.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='3790\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n[33] Li, S., Park, S., Lee, I., & Bastani, O. (2023). “TRAC: Trustworthy Retrieval Augmented Chatbot,” arXiv preprint arXiv:2307.04642. \\n[34] Lozano, A., Fleming, S. L., Chiang, C. C., & Shah, N. (2023). “Clinfo. ai: An open-source retrieval-augmented large language model system \\nfor answering medical questions using scientific literature,” In Pacific symposium on Biocomputing 2024: 8-23.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='[35] Ding, W., Cao, Y., Zhao, D., Xiao, C., & Pavone, M. (2023). “RealGen: Retrieval Augmented Generation for Controllable Traffic \\nScenarios,” arXiv preprint arXiv:2312.13303. \\n[36] Thompson, W. E., Vidmar, D. M., De Freitas, J. K., Pfeifer, J. M., Fornwalt, B. K., Chen, R., ... & Miotto, R. (2023). “Large Language \\nModels with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,” arXiv preprint arXiv:2312.06457.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='[37] Wang, W., Wang, Y., Joty, S., & Hoi, S. C. (2023). “Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program \\nrepair,” In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software \\nEngineering: 146-158. \\n[38] Guo, Y., Li, Z., Jin, X., Liu, Y., Zeng, Y., Liu, W., ... & Cheng, X. (2023). “Retrieval-augmented code generation for universal information \\nextraction,” arXiv preprint arXiv:2311.02962.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='extraction,” arXiv preprint arXiv:2311.02962. \\n[39] Kagaya, T., Yuan, T. J., Lou, Y., Karlekar, J., Pranata, S., Kinose, A., ... & You, Y. (2024). “RAP: Retrieval-Augmented Planning with \\nContextual Memory for Multimodal LLM Agents,” arXiv preprint arXiv:2402.03610. \\n[40] Fan, R. Z., Fan, Y., Chen, J., Guo, J., Zhang, R., & Cheng, X. (2023). “RIGHT: Retrieval-augmented Generation for Mainstream Hashtag \\nRecommendation,” arXiv preprint arXiv:2312.10466.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='Recommendation,” arXiv preprint arXiv:2312.10466. \\n[41] Jiang, S., Tang, W., Chen, X., Tanga, R., Wang, H., & Wang, W. (2023). Raucg: Retrieval-augmented unsupervised counter narrative \\ngeneration for hate speech. arXiv preprint arXiv:2310.05650. \\n[42] Xu, R., Yu, Y., Ho, J., & Yang, C. (2023). “Weakly-supervised scientific document classification via retrieval-augmented multi-stage'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='training,” In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval: 2501-\\n2505. \\n[43] Hu, M., Zhao, X., Wei, J., Wu, J., Sun, X., Li, Z., ... & Zhang, Y. (2023). “rT5: A Retrieval-Augmented Pre-trained Model for Ancient \\nChinese Entity Description Generation,” In International Conference on NLP and Chinese Computing. Cham: Springer: 736-748.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='[44] Song, S., Lv, Q., Geng, L., Cao, Z., & Fu, G. (2023). “RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling \\nCheck,” In CCF International Conference on Natural Language Processing and Chinese Computing. Cham: Springer: 551-562.  \\n[45] Shi, P., Zhang, R., Bai, H., & Lin, J. (2022). “Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql \\nsemantic parsing,” arXiv preprint arXiv:2210.13693.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='semantic parsing,” arXiv preprint arXiv:2210.13693. \\n[46] Asai, A., Wu, Z., Wang, Y., Sil, A., & Hajishirzi, H. (2023). “Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection,” arXiv preprint arXiv:2310.11511. \\n[47] Lin, D. (2024). “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,” arXiv preprint \\narXiv:2401.12599.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='arXiv:2401.12599. \\n[48] He, X., Tian, Y., Sun, Y., Chawla, N. V., Laurent, T., LeCun, Y., ... & Hooi, B. (2024). “G-Retriever: Retrieval-Augmented Generation for \\nTextual Graph Understanding and Question Answering,” arXiv preprint arXiv:2402.07630. \\n[49] Ahmad, S. R. (2024). “Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model \\nImplementation for Multicultural Enterprise,” arXiv preprint arXiv:2401.01511.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='Implementation for Multicultural Enterprise,” arXiv preprint arXiv:2401.01511. \\n[50] Zhao, C., Jiang, Y., Qiu, Y., Zhang, H., & Yang, W. Y. (2023). “Differentiable Retrieval Augmentation via Generative Language Modeling \\nfor E-commerce Query Intent Classification,” In Proceedings of the 32nd ACM International Conference on Information and Knowledge \\nManagement: 4445-4449.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='Management: 4445-4449. \\n[51] Nakhod, o. Using retrieval-augmented generation to elevate low-code developer skills. https://doi.org/10.15407/jai2023.03.126 \\n[52] Wang, H., Huang, W., Deng, Y., Wang, R., Wang, Z., Wang, Y., ... & Wong, K. F. (2024). “UniMS-RAG: A Unified Multi-source \\nRetrieval-Augmented Generation for Personalized Dialogue Systems,” arXiv preprint arXiv:2401.13256.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='[53] Du, X., & Ji, H. (2022). “Retrieval-augmented generative question answering for event argument extraction,” arXiv preprint \\narXiv:2211.07067. \\n[54] Ranade, P., & Joshi, A. (2023). “FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction,” arXiv \\npreprint arXiv:2310.13848. \\n[55] Zhang, Z., Fang, M., & Chen, L. (2024). “RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='Question Answering,” arXiv preprint arXiv:2402.16457. \\n[56] Zhang, S., Yadav, D., & Jin, T. (2023). “Cash transaction booking via retrieval augmented LLM. KDD 2023 Workshop on Robust NLP for \\nFinance (RobustFin),” https://www.amazon.science/publications/cash-transaction-booking-via-retrieval-augmented-llm \\n[57] Pouplin, T., Sun, H., Holt, S., & Van der Schaar, M. (2024). “Retrieval-Augmented Thought Process as Sequential Decision Making,” arXiv \\npreprint arXiv:2402.07812.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='preprint arXiv:2402.07812. \\n[58] Munikoti, S., Acharya, A., Wagle, S., & Horawalavithana, S. (2023). “ATLANTIC: Structure-Aware Retrieval-Augmented Language Model \\nfor Interdisciplinary Science,” arXiv preprint arXiv:2311.12289. \\n[59] Markey, N., El-Mansouri, I., Rensonnet, G., van Langen, C., & Meier, C. (2024). “From RAGs to riches: Using large language models to \\nwrite documents for clinical trials,” arXiv preprint arXiv:2402.16406.'),\n",
              "  Document(metadata={'page': 9, 'type': 'text'}, page_content='write documents for clinical trials,” arXiv preprint arXiv:2402.16406. \\n[60] Kim, J., & Min, M. (2024). “From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process,” arXiv \\npreprint arXiv:2402.01717.')],\n",
              " array([[ 0.03966911,  0.02233979,  0.03567739, ..., -0.02313417,\n",
              "         -0.04350961, -0.0356045 ],\n",
              "        [-0.00800639, -0.01203629, -0.00161855, ...,  0.00660626,\n",
              "          0.00440606, -0.06144243],\n",
              "        [ 0.00099998,  0.01867807, -0.02668937, ..., -0.09358583,\n",
              "         -0.03183691,  0.03085243],\n",
              "        ...,\n",
              "        [ 0.07239746, -0.04516119, -0.02439865, ..., -0.04424017,\n",
              "         -0.04348588, -0.01945706],\n",
              "        [-0.00251454, -0.04437987, -0.01670402, ..., -0.03156574,\n",
              "         -0.08298665, -0.03596834],\n",
              "        [-0.0059841 , -0.00655963, -0.03402936, ..., -0.03022855,\n",
              "         -0.0441472 , -0.02386659]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom FAISS index since we have precomputed embeddings\n",
        "vector_store = FAISS.from_embeddings(\n",
        "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
        "    embedding=None,  # We're using precomputed embeddings\n",
        "    metadatas=[doc.metadata for doc in all_docs]\n",
        ")\n",
        "vector_store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUxQW-FXOZ5w",
        "outputId": "06bb4253-e93a-4ea4-ea47-764ec4a4a18f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7df2f1b39d10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nZ2zT_KOe03",
        "outputId": "e38b2668-1983-46ac-c51a-08248d225ee9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash-latest', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7df2ee14e050>, default_metadata=(), model_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_multimodal(query, k=5):\n",
        "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
        "    # Embed query using CLIP\n",
        "    query_embedding = embed_text(query)\n",
        "\n",
        "    # Search in unified vector store\n",
        "    results = vector_store.similarity_search_by_vector(\n",
        "        embedding=query_embedding,\n",
        "        k=k\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ZMAfjD0GOev3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multimodal_message(query, retrieved_docs):\n",
        "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
        "    content = []\n",
        "\n",
        "    # Add the query\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
        "    })\n",
        "\n",
        "    # Separate text and image documents\n",
        "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
        "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
        "\n",
        "    # Add text context\n",
        "    if text_docs:\n",
        "        text_context = \"\\n\\n\".join([\n",
        "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
        "            for doc in text_docs\n",
        "        ])\n",
        "        content.append({\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
        "        })\n",
        "\n",
        "    # Add images\n",
        "    for doc in image_docs:\n",
        "        image_id = doc.metadata.get(\"image_id\")\n",
        "        if image_id and image_id in image_data_store:\n",
        "            content.append({\n",
        "                \"type\": \"text\",\n",
        "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
        "            })\n",
        "            content.append({\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # Add instruction\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
        "    })\n",
        "\n",
        "    return HumanMessage(content=content)"
      ],
      "metadata": {
        "id": "Oyg27cBiOerf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multimodal_pdf_rag_pipeline(query):\n",
        "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    context_docs = retrieve_multimodal(query, k=5)\n",
        "\n",
        "    # Create multimodal message\n",
        "    message = create_multimodal_message(query, context_docs)\n",
        "\n",
        "    # Get response from GPT-4V\n",
        "    response = llm.invoke([message])\n",
        "\n",
        "    # Print retrieved context info\n",
        "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
        "    for doc in context_docs:\n",
        "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"?\")\n",
        "        if doc_type == \"text\":\n",
        "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
        "            print(f\"  - Text from page {page}: {preview}\")\n",
        "        else:\n",
        "            print(f\"  - Image from page {page}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "uTEctoC6Oekn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        #\"What does the chart on page 1 show about revenue trends?\",\n",
        "        #\"Summarize the main findings from the document\",\n",
        "        \"What visual elements are present in the document?\",\n",
        "        \"What is the abstract ot the paper?\",\n",
        "        \"QWhat are those components of the paper?\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 50)\n",
        "        answer = multimodal_pdf_rag_pipeline(query)\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFarvex8OeTQ",
        "outputId": "c3fb3a7a-53a9-423f-9064-a5338654d572"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What visual elements are present in the document?\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 5 documents:\n",
            "  - Text from page 7: 3) Educational \n",
            "- Educational decision making [10] \n",
            "- Textbook QA [11] \n",
            "4) Technology and Software D...\n",
            "  - Text from page 1: articles, the system can respond to current business events, presenting opportunities for business i...\n",
            "  - Text from page 7: 1) Medical / Biomedical \n",
            "- Biomedical QA [1] \n",
            "- Medical QA [3] \n",
            "- Medical text summarization [4] \n",
            "- ...\n",
            "  - Text from page 6: 4) Text Analysis and Processing \n",
            "- Sentiments classification [13] \n",
            "- Text error correction [35] \n",
            "- T...\n",
            "  - Text from page 6: Text Generation and Summarization: (6) \n",
            "Information Retrieval and Extraction: (6) \n",
            "Text Analysis and...\n",
            "\n",
            "\n",
            "Answer: Based on the provided text, the only visual element explicitly mentioned is a figure:  \"Fig. 1. (a) A generic RAG architecture, where users’ queries, potentially in different modalities (e.g., text, code, image, etc.), are inputted into\".  The figure itself is not described, only its caption.  We know it depicts a RAG architecture and accepts different input modalities (text, code, image, etc.), but no further visual details are given.\n",
            "======================================================================\n",
            "\n",
            "Query: What is the abstract ot the paper?\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 5 documents:\n",
            "  - Text from page 7: 3) Educational \n",
            "- Educational decision making [10] \n",
            "- Textbook QA [11] \n",
            "4) Technology and Software D...\n",
            "  - Text from page 7: 1) Medical / Biomedical \n",
            "- Biomedical QA [1] \n",
            "- Medical QA [3] \n",
            "- Medical text summarization [4] \n",
            "- ...\n",
            "  - Text from page 1: applications. Since the RAG with LLM domain is relatively new and emerging, with many studies availa...\n",
            "  - Text from page 4: S2ORC dataset \n",
            "Science QA and scientific \n",
            "document classification  \n",
            "50 \n",
            "Writing documents for clinic...\n",
            "  - Text from page 6: listed under various headings in Table 2. \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "1) Question Answering (QA)  \n",
            "- Biomedical QA [1] ...\n",
            "\n",
            "\n",
            "Answer: The provided text does not contain an abstract.  The excerpts show sections of a paper listing applications of a technology (likely Retrieval Augmented Generation with Large Language Models, based on the mention on page 1), categorized by domain (Medical/Biomedical, Financial, Educational, Technology and Software Development, Social and Communication).  There is no summary paragraph that would constitute an abstract.\n",
            "======================================================================\n",
            "\n",
            "Query: QWhat are those components of the paper?\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 5 documents:\n",
            "  - Text from page 7: 3) Educational \n",
            "- Educational decision making [10] \n",
            "- Textbook QA [11] \n",
            "4) Technology and Software D...\n",
            "  - Text from page 7: 1) Medical / Biomedical \n",
            "- Biomedical QA [1] \n",
            "- Medical QA [3] \n",
            "- Medical text summarization [4] \n",
            "- ...\n",
            "  - Text from page 6: 4) Text Analysis and Processing \n",
            "- Sentiments classification [13] \n",
            "- Text error correction [35] \n",
            "- T...\n",
            "  - Text from page 1: articles, the system can respond to current business events, presenting opportunities for business i...\n",
            "  - Text from page 6: Text Generation and Summarization: (6) \n",
            "Information Retrieval and Extraction: (6) \n",
            "Text Analysis and...\n",
            "\n",
            "\n",
            "Answer: Based on the provided text, the paper's components are categorized into several areas:\n",
            "\n",
            "* **Medical/Biomedical:**  This includes biomedical QA, medical QA, medical text summarization, health education QA, identifying diseases, clinical decision-making, clinical-related writing, science QA, scientific document classification, and pharma industry regulatory compliance QA.\n",
            "\n",
            "* **Financial:** This includes financial QA and automated cash transaction booking.\n",
            "\n",
            "* **Educational:** This includes educational decision-making and textbook QA.\n",
            "\n",
            "* **Technology and Software Development:** This encompasses table QA, technical product information QA, software development and maintenance, generative QA and informative conversations, information extraction, text error correction, text-to-SQL translation, personalized dialogue systems, and event argument (answer) extraction.  Note that some of these are also listed under separate categories (e.g., \"Software Development and Maintenance\").\n",
            "\n",
            "* **Social and Communication:**  This category is mentioned but no specific sub-components are detailed in the provided text.\n",
            "\n",
            "* **Text Analysis and Processing:** This includes sentiment classification, text error correction, text-to-SQL translation, scientific document classification, and combating online hate speech.\n",
            "\n",
            "* **Decision Making and Applications:** This includes clinical decision-making and educational decision-making.  (This overlaps with other categories).\n",
            "\n",
            "* **Text Generation and Summarization:**  This category is mentioned but no specific sub-components are detailed in the provided text.\n",
            "\n",
            "\n",
            "* **Information Retrieval and Extraction:** This category is mentioned but no specific sub-components are detailed in the provided text.\n",
            "\n",
            "\n",
            "* **Other Categories:** This is a catch-all category with unspecified components.\n",
            "\n",
            "The numbers in parentheses after each category on page 6 seem to represent the number of sub-components within that category, though the exact correspondence isn't perfectly clear from the provided snippets.  The inconsistency in category names and organization across pages suggests that the provided excerpts are not a complete representation of the paper's structure.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcANTNBnTDxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}